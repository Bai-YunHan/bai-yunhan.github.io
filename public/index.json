[{"content":"Source: Stanford CS231N Deep Learning for Computer Vision | Spring 2025 | Lecture 13: Generative Models 1 (link)\nImplementation Pseudo Code Encoder The input image dimension is [$B$, $C$, $H$, $W$] in this case is [1, 28, 28], where 28x28 is the image size of MNIST data. The original classification head (fc) is replaced to produce the mean (mu) and log-variance (logvar) required for the VAE\u0026rsquo;s reparameterization trick. The flow of encoder:Image â†’ Modified ResNet â†’ Project â†’ Chunk â†’ mu, logvar $[1, 28, 28]$ $\\xrightarrow{\\displaystyle \\textsf{ResNet}}$ [1, 1024] $\\xrightarrow{\\displaystyle \\textsf{Project}}$ $[1, 256]$ $\\xrightarrow{\\displaystyle \\textsf{Chunk}}$ $([1, 128], [1, 128])$ Decoder The flow of decoder: Project -\u0026gt; Reshape -\u0026gt; Nx Conv+Up The input to the decoder is the latent code $z$. The latent dimension is [$B$, $D_{in}$] ($D_{in}$ default to 128) First it passes through a MLP â†’ [$B$, $D_{out}$] Reshape from [$B$, $D_{out}$] â†’ $[B, C_{init}, H_{init}, W_{init}]$, where $H_{init}=W_{init}$ Then it passes through N layers of Convolution + Up-sampleing layer â†’ $[B, 1, H_{out}, W_{out}]$, where $H_{out}=W_{out}=28$ VAE The VAE takes data from MNIST dataset then pass it through a ResNet encoder. Then sample latent code $z$ using $z = \\mu +\\sigma\\cdot\\epsilon$ where $\\epsilon \\sim N(0, I)$. Then pass $z$ to the decoder. Training Run input data through encoder to get distribution over $z$. Prior loss: Encoder output should be unit Gaussian (zero mean, unit variance). Sample $z$ from encoder output $q_\\phi(z\\mid x)$ (Reparameterization trick). Run $z$ through decoder to get predicted data mean. Reconstruction loss: predicted mean should match $x$ in L2. Elaboration 1. The Core Concept Unlike standard Autoencoders which mapÂ ImageÂ $\\to$Â CodeÂ $\\to$Â Image, a VAE mapsÂ ImageÂ $\\to$Â Distribution ParametersÂ $\\to$Â Image.\nThe networks do not output probabilities directly; they output theÂ parametersÂ (MeanÂ $\\mu$Â and Standard DeviationÂ $\\sigma$) of Gaussian distributions.\n2. The Encoder (Inference Model) The encoder compresses high-dimensional data into a low-dimensional latent space.\nInput:Â Image of shapeÂ $3 \\times H \\times W$.\nOutput:Â Two vectors, both of lengthÂ $D$Â (the latent dimension, e.g., 128).\nMean Vector ($\\mu_z$):Â The center of the latent distribution. Log-Variance Vector ($\\log \\sigma^2_z$):Â The spread of the distribution. Design Choice (Diagonal Covariance):\nWe assume the dimensions of $z$ are statistically independent. Instead of predicting a full $D \\times D$ covariance matrix (which would handle correlations between features), we only predict the diagonal.\nIntuition:Â This drastically reduces parameters from quadratic ($D^2$) to linear ($D$), making the model easier to train.\n3. The Reparameterization Trick The reparameterization trick is a tactical solution to a technical problem: How do we backpropagate through a random node?\nThe Issue: Inside the network, we need to sample $z$ from the distribution $q_\\phi(z|x)$ (typically a Gaussian with mean $\\mu$ and variance $\\sigma^2$. Standard random sampling breaks the chain of derivatives needed for backpropagation. The Trick: We move the randomness to an external variable $\\epsilon$ that is independent of the model parameters. The Equation: Instead of sampling $z \\sim N(\\mu, \\sigma^2)$ directly, we calculate: $z = \\mu + \\sigma \\odot \\epsilon$ where $\\epsilon \\sim N(0, 1)$ (standard normal distribution). This allows gradients to flow through $\\mu$ and $\\sigma$ during training, making the VAE end-to-end differentiable.\n4. The Decoder (Generative Model) The decoder reconstructs the image from a sampled latent point.\nInput:Â A vectorÂ $z$Â of lengthÂ $D$Â (sampled from the Encoder\u0026rsquo;s distribution).\nOutput:Â A tensor of shapeÂ $3 \\times H \\times W$Â (same as input image).\nWhat this output represents:\nMathematically, this output is the Mean Vector ($\\mu_x$) of the pixel probability distribution.\nDesign Choice (Fixed Variance):\nWe assume the pixel distribution is a Gaussian with a fixed standard deviation of 1 ($\\sigma=1$) and spherical covariance (no correlations between pixels).\n5. Critical Intuition: Why Design it This Way? A. Why Diagonal/Fixed Covariance? (The \u0026ldquo;Unmanageable Size\u0026rdquo; Problem)\nIf the Decoder tried to learn the correlations between every pair of pixels (Full Covariance), it would need a matrix of sizeÂ $(H \\cdot W)^2$. For a small image, this is millions of parameters; for large images, trillions. Solution:Â By assuming pixels are independent (Diagonal) or fixed (Spherical), we reduce complexity fromÂ QuadraticÂ toÂ Linear. B. Why NOT Predict Separate Variances for Each Pixel? (The \u0026ldquo;Cheating\u0026rdquo; Problem)\nThe Idea:Â Why not let the decoder output a specific varianceÂ $\\sigma_i$Â for every pixelÂ $i$?\nThe Cheating Mechanism:\nAs derived in the Appendix, the Loss function includes two competing terms:\n$$ \\text{Loss} \\approx \\underbrace{\\frac{(x - \\mu)^2}{2\\sigma^2}}_{\\text{Reconstruction Error}} + \\underbrace{\\log(\\sigma)}_{\\text{Uncertainty Penalty}} $$ How it cheats:Â For difficult details (like edges), the reconstruction errorÂ $(x-\\mu)^2$Â is naturally high. To minimize the Loss, the model can simply predict aÂ massive variance ($\\sigma \\to \\infty$). The Result:Â A hugeÂ $\\sigma$Â crushes the Reconstruction Error term to near zero. It is \u0026ldquo;cheaper\u0026rdquo; for the model to admit total uncertainty than to learn the hard feature. The Fix:Â By forcingÂ $\\sigma=1$, the denominator is constant. The modelÂ mustÂ minimizeÂ $(x - \\mu)^2$Â to lower the loss.\nC. Why is the \u0026ldquo;Reconstructed Image\u0026rdquo; just the Mean? (The MSE Connection)\nWhen you maximize the likelihood of a Gaussian where $\\sigma$ is fixed to 1, the math simplifies to:\n$\\text{Maximize } \\log P(x|z) \\iff \\text{Minimize } (x - \\mu)^2$\nTherefore, the outputÂ $\\mu$Â is the value that minimizes theÂ Mean Squared Error (L2 Loss).\nMotivation of VAE While standard auto-encoders are effective for learning feature representations (via reconstruction), they fail as generative models because their latent space ($Z$) has no enforced structure. starting atÂ [51:18].\nThe Limitation of Autoencoders [51:18]:Â if you want to use a standard autoencoder to generateÂ newÂ data, you would need to throw away the encoder and sample a latent vectorÂ $Z$Â to pass through the decoder. However, because the autoencoder places no constraints on the latent space, you have no idea what the distribution of validÂ $Z$Â vectors looks like. \u0026ldquo;Kicking the Can Down the Road\u0026rdquo; [51:57]:Â He uses this phrase to describe the problem: just as we didn\u0026rsquo;t know the distribution of the original dataÂ $X$, we now don\u0026rsquo;t know the distribution of the latent vectorsÂ $Z$. Therefore, we are \u0026ldquo;stuck\u0026rdquo; because we cannot easily sample a valid code to generate a realistic image. The VAE Solution [52:14]:Â The motivation for the VAE is to \u0026ldquo;force some structure on the $Z$\u0026rsquo;s.\u0026rdquo; By forcing the latent space to approximate a known distribution (typically a unit Gaussian), the VAE ensures that weÂ canÂ easily sample a randomÂ $Z$Â from that known distribution and pass it through the decoder to generate valid new data. VAEs are essentially a \u0026ldquo;probabilistic spin\u0026rdquo; on traditional autoencoders designed specifically to enable this sampling capabilityÂ [53:08].\nFormulation Goal â€” Maximize Marginal Likelihood We want to find parametersÂ $\\theta$Â such that the data we observed ($x$) is highly probable under our model. We maximize theÂ Marginal Likelihood, not the Conditional Likelihood.\nEquation: $\\theta^* = \\operatorname*{argmax}_\\theta \\sum_{i} \\log p_\\theta(x^{(i)})$\nWhy Marginal $p_\\theta(x)$: The \u0026ldquo;marginal\u0026rdquo; integrates overÂ all possibleÂ latent variablesÂ $z$.\n$p(x\\mid z)$ (Single Case): This asks, \u0026ldquo;How likely is this image if the hidden concept is exactly $z$?\u0026rdquo; $p(x)$ (Collectively all cases): This asks, \u0026ldquo;How likely is this image considering every possible hidden concept that could produce it?\u0026rdquo; i.e. $p(x) = \\int p(x|z)p(z)dz$. Symbol Name Description $p_\\theta(x \\mid z)$ Likelihood How likely is the imageÂ $x$, given the hidden traitsÂ $z$? $p_\\theta(z)$ Prior What is the distribution of hidden traitsÂ $z$Â before seeing data? $p_\\theta(z \\mid x)$ Posterior Given the imageÂ $x$, what are the likely traitsÂ $z$? $p_\\theta(x)$ Marginal Likelihood How probable is the dataÂ $x$Â overall, summing over all possibleÂ $z$\u0026rsquo;s? Why $p_\\theta(x)$ is called marginal likelihood?\nHistorically, probability distributions were written in tables. For example, if you had a joint distribution over $x$ and $z$:\nz=0 z=1 Row total x=0 â€¦ â€¦ p(x=0) x=1 â€¦ â€¦ p(x=1) Column total p(z=0) p(z=1) 1 To get the probability of $x$ alone, you would sum across the row, and the result would appear in the margin of the table. Hence:\nğŸ”¹ Summing out a variable = looking at the margin of the joint distribution. That operation was literally written in the margins of the table â†’ â€œmarginal probability.â€ The process of computing it â†’ â€œmarginalization.â€\nBlocker â† The Intractability To calculate the marginal likelihood, we face a mathematical dead end because we cannot compute the integral or the posterior.\nThe Integral Problem: $p_\\theta(x) = \\int p_\\theta(x|z)p(z) \\, dz$\nFor complex data (like images), this integral is impossible to calculate (intractable) because it requires summing over an infinite number of possible $z$ configurations.\nThe Bayes\u0026rsquo; Rule Problem:\nWe might try to find $p_\\theta(x)$ via Bayes\u0026rsquo; Rule:\n$$ p_\\theta(x) = \\frac{p_\\theta(x\\mid z)p(z)}{p_\\theta(z\\mid x)} $$However, we only have a decoder to compute $p_\\theta(x\\mid z)$. To compute the denominator $p_\\theta(z|x)$ (the true posterior), we need to use the Bayesâ€™ rule which requires knowing $p_\\theta(x)$, $p_\\theta(z\\mid x) = \\frac{p_\\theta(x\\mid z)p(z)}{p_\\theta(x)}$, so it is intractable.\nSolution â† Variational Inference (ReplacingÂ $p_\\theta$Â withÂ $q_\\phi$) Since the true posteriorÂ $p_\\theta(z|x)$Â is impossible to calculate, weÂ approximateÂ it with a tractable distributionÂ $q_\\phi(z|x)$Â (the Encoder/Neural Network).\nThe Approximation: $q_\\phi(z|x) \\approx p_\\theta(z|x)$ Intuition The reconstruction loss and prior loss is fighting with each other This \u0026ldquo;tug-of-war\u0026rdquo; is one of the most fascinating parts of a VAE because you can actuallyÂ seeÂ the result of these two forces fighting when you plot the latent space.\nHere is the visual evidence that the KL divergence doesn\u0026rsquo;t just \u0026ldquo;force\u0026rdquo; everything to zero, but rather organizes it.\n1. The \u0026ldquo;Healthy\u0026rdquo; VAE Balance When the Reconstruction Loss (keep data distinct) and KL Loss (keep data Gaussian) are balanced correctly, the latent space looks like this:\nWhat to notice:\nGlobal Centering:Â Notice that theÂ entire cloudÂ of points is centered aroundÂ $(0,0)$Â and roughly spans betweenÂ $-3$ andÂ $3$Â (typical for a unit variance). This is the KL loss doing its job. Local Distinctness:Â Crucially, individual digits areÂ notÂ all collapsed toÂ $(0,0)$. The \u0026ldquo;7\u0026quot;s might be clustered atÂ $(-1, 2)$Â and the \u0026ldquo;0\u0026quot;s atÂ $(1, -1)$. The encoder has learned to shift the meanÂ $\\mu$Â awayÂ from 0 just enough to distinguish the digits, but keeps them packed tight enough to satisfy the Gaussian prior. Smoothness:Â The clusters touch each other. This means if you sample a point halfway between a \u0026ldquo;1\u0026rdquo; and a \u0026ldquo;7\u0026rdquo;, you get a digit that looks like a plausible mix of both. 2. What happensÂ withoutÂ the KL \u0026ldquo;Force\u0026rdquo; (Standard Autoencoder) If you remove the KL term (essentially predicting mean/variance but with no penalty for where they are), the Reconstruction Loss takes over completely.\nWhat to notice:\nGaps and Explosions:Â The clusters fly apart. The model might put \u0026ldquo;0\u0026quot;s atÂ $(100, 100)$Â and \u0026ldquo;1\u0026quot;s atÂ $(-50, -50)$Â just to be absolutely sure it doesn\u0026rsquo;t confuse them. No Structure:Â There is no center. The spread is arbitrary. Dead Zones:Â There are massive empty gaps between clusters. If you try to generate an image from those gaps, you get static/noise because the decoder has never seen data there. 3. What happens if the KL Force Wins (Posterior Collapse) This is the scenario you fearedâ€”where the model is \u0026ldquo;forced\u0026rdquo; to 0 and 1.\nThe Visual:Â Imagine the first plot, but all the colored clusters are stacked directly on top of each other atÂ $(0,0)$. The Result:Â The encoder output is alwaysÂ $\\mu=0, \\sigma=1$Â effectively ignoring the input image. The decoder receives pure noise every time and produces a single, blurry \u0026ldquo;average\u0026rdquo; image (like a gray ghost) for every single input. 4. Summary The KL divergence acts like aÂ springÂ attached to the originÂ $(0,0)$.\nReconstructionÂ tries to pull the data points apart so they don\u0026rsquo;t overlap. KL DivergenceÂ (the spring) pulls them back toward the center. The final state is a tense equilibrium: data points distinct enough to be recognized, but bunched tight enough to form a smooth, continuous space. Role of $z_i$ in latent code $Z$ Auto-Encoding Variational Bayes, ICLR 2014.\nIn a Variational Autoencoder (VAE),Â $Z$Â represents the latent codeâ€”a compressed, hidden representation of the input data.Â $z_i$Â refers to a specific individual dimension (or component) within this vector. The core idea shown here isÂ disentanglement: the model attempts to map distinct, meaningful semantic features of the data to separate dimensions ($z_i$). For example, changing the value of one dimension ($z_1$) might smoothly transform the digit\u0026rsquo;s identity (e.g., changing a 6 to a 0), while changing another dimension ($z_2$) might strictly alter the slant or thickness of the writing, without changing the digit itself.\nImplementation of sampling $z_1$ and $z_2$ and set rest of $z_i$ to zeros.\nLimitation: Blurry results (i.e. Whatâ€™s Next?) The nature of loss function (MSE/log-likelihood) tend to produce \u0026ldquo;blurry\u0026rdquo; results in VAE.\nThe \u0026ldquo;blurry VAE\u0026rdquo; phenomenon is a direct mathematical consequence of how we measure \u0026ldquo;error\u0026rdquo; usingÂ Mean Squared Error (MSE)Â orÂ Log-Likelihood.\nIn short:Â MSE forces the model to be a conservative \u0026ldquo;average,\u0026rdquo; rather than a bold \u0026ldquo;guesser.\u0026rdquo;\nHere is the breakdown of why this happens.\n1. The \u0026ldquo;Safety\u0026rdquo; of the Average Imagine the model is trying to reconstruct a picture of a zebra, but the latent space is slightly uncertain about exactly where a specific stripe should be.\nPossibility A:Â The stripe is at pixel 100. Possibility B:Â The stripe is at pixel 101. If the model guesses A (sharp stripe at 100), but the truth was B, the MSE penalty is massive because the pixel values are totally opposite (black vs. white).\nIf the model guesses B (sharp stripe at 101), but the truth was A, the penalty is essentially double (wrong on both pixels).\nThe VAE\u0026rsquo;s Solution:Â It predicts aÂ gray smearÂ across pixels 100 and 101.\nWhy?Â Gray is never \u0026ldquo;perfectly right,\u0026rdquo; but it is never \u0026ldquo;catastrophically wrong.\u0026rdquo; It minimizes theÂ squaredÂ error across all plausible possibilities. The model learns toÂ hedge its betsÂ to lower the loss. 2. The Unimodal Assumption (Mathematics) This is the technical root of the problem.\nMSE is equivalent to Maximum Likelihood under a Gaussian distribution. When you use MSE, you are implicitly telling the model:Â \u0026ldquo;Assume the pixel value comes from a single Bell curve (Unimodal).\u0026rdquo; The Problem: Real data is Multimodal.\nA pixel at the edge of an object could plausibly be Black (background) OR White (object). It is almost never Gray.\nMultimodal Reality:Â Two peaks (one at 0, one at 255). Unimodal Constraint:Â The model must fitÂ oneÂ Bell curve to explain both peaks. Result:Â The model centers the Bell curve right in the middle (Gray/Blur) to cover both options. 3. High Frequency vs. Low Frequency Low Frequency (Structure):Â Where is the head? Where is the background? VAEs are great at this because the \u0026ldquo;average\u0026rdquo; of a head is still roughly a head shape. High Frequency (Texture/Edges):Â Where is this specific hair strand? Where is the pore on the skin? The \u0026ldquo;average\u0026rdquo; of many possible hair strand positions is just a smooth blur. Since VAEs optimize for the average case, they effectively apply aÂ low-pass filterÂ to the image, smoothing out all the sharp \u0026ldquo;high frequency\u0026rdquo; noise that our eyes interpret as realistic detail. Comparison: Why GANs don\u0026rsquo;t blur VAE (MSE):Â \u0026ldquo;I must be close to the pixel values on average. I will be safe and blurry.\u0026rdquo; GAN (Discriminator):Â \u0026ldquo;I don\u0026rsquo;t care if the stripe is at pixel 100 or 101, but if it\u0026rsquo;s gray/blurry, the Discriminator will know it\u0026rsquo;s fake. I must pickÂ oneÂ sharp location, even if I guess wrong.\u0026rdquo; Loss Function Strategy Result MSE / L2 Minimize variance; fit the mean of the distribution. Blurry.Â The mean of \u0026ldquo;sharp left\u0026rdquo; and \u0026ldquo;sharp right\u0026rdquo; is \u0026ldquo;blurry middle.\u0026rdquo; L1 Loss Minimize absolute error; fit theÂ median. Slightly sharper, but still blurry compared to GANs. Adversarial Fool a judge; match theÂ distribution. Sharp.Â Forces a decision (collapse to a mode) rather than an average. The Perceptual Loss (VGG Loss)Â orÂ VQ-VAE are the standard modern techniques specifically designed to fix this blurriness in VAEs without needing a full GAN setup.\nQ \u0026amp; A: Question-1 Please validate my understanding of VAE below: We sample $z$ from the encoder, $z \\sim q_\\phi(z|x)$. $q_\\phi(z|x)$ is a gaussian distribution, i.e. $z \\sim N(\\mu, \\sigma^2)$. We use KL divergence loss to make $N(\\mu, \\sigma^2)$ look similar to a standard gaussian distribution. $z$ is a latent code (vector of length $D$). Say, we are reconstructing hand written digit of 10 classes (digit 0 ~ 9). The $z$ representing digit $0$ is a gaussian distribution, and the $z$ representing digit $1$ is another gaussian distribution. These cluster both locate near to zero mean. The totality of the 10 cluster conform to the standard gaussian distribution. The distinctiveness between each cluster comes from the difference in $\\mu$ and $\\sigma$ generated by the encoder. is my understanding correct?\nAnswer:\nMy understanding are confirmed to be correct.\nIn summary, the VAE is a Master of Compromise\nThe VAE doesÂ clusteringÂ (via the reconstruction term), where each class (digit) gets its own localized cluster defined by its meanÂ $\\mu$. The VAE doesÂ regularizationÂ (via the KL term), ensuring that all these clusters stay packed together in a smooth, continuous space that collectively resembles theÂ $N(\\mu, \\sigma^2)$ distribution. The Local View (Individual Input) For any single input (e.g.,Â a specific image of a digit \u0026lsquo;2\u0026rsquo;),Â the encoder outputs a specificÂ $\\mu$Â andÂ $\\sigma$. This defines aÂ \u0026ldquo;neighborhood\u0026rdquo;Â in the latent space where that specific image lives. By samplingÂ $z$Â from this neighborhood,Â the decoder learns that any point in this small area should look likeÂ that specificÂ \u0026lsquo;2\u0026rsquo;. The Class View (Group of Inputs belong to same class) As you noted,Â all the \u0026lsquo;2\u0026rsquo;s will have their ownÂ $\\mu$Â andÂ $\\sigma$Â values. Because they all share similar visual features,Â the Reconstruction Loss naturally forces theirÂ $\\mu$Â values to be near each other. This creates aÂ clusterÂ (a Gaussian Mixture component) for the digit \u0026lsquo;2\u0026rsquo;. The Global View (The Prior) TheÂ KL DivergenceÂ is the \u0026ldquo;global supervisor.\u0026rdquo; It doesn\u0026rsquo;t care about the labels (0â€“9); it only sees the totality of all these neighborhoods. It exerts a pull onÂ everyÂ individual distribution to stay close toÂ $0$Â and have a spread nearÂ $1$. The result is that the \u0026ldquo;cloud of clusters\u0026rdquo; (the totality) conforms to the standard Gaussian shape. A Helpful Mental Model: \u0026ldquo;The Bubble Map\u0026rdquo; Imagine each input image is aÂ bubble.\nTheÂ Reconstruction LossÂ wants the bubbles to be solid and distinct so the decoder can \u0026ldquo;see\u0026rdquo; the image clearly. TheÂ KL LossÂ wants all the bubbles to move to the center of the map and be exactly the same size. The Training ProcessÂ is the struggle to pack all these bubbles into a small,Â circular container (the Standard Normal Prior) without them overlapping so much that they lose their identity. Question-2 Say, we are reconstructing hand written digit of 10 classes (digit 0 ~ 9).\nRegarding the encoder, when given 10 input images of digit â€˜1â€™, the output $\\mu$ and $\\sigma$ would be very close if not the same. Namely, the latent code $z$ of the image of digit â€˜1â€™ belong to the same cluster. am i right?\nDuring inference, if we feed the same image of â€˜1â€™ to the encoder, though the $\\mu$ and $\\sigma$ output by encoder is the same, due to the randomness of $\\epsilon$, the output $z$ would be different. namely, the latent code for the same input image is not deterministic. Am i right? In contradiction, for auto-encoder, the latent code for the same input image is deterministic. What is the use of the randomness?\nAnswer:\nLet\u0026rsquo;s validate your points and then dive into the \u0026ldquo;why\u0026rdquo; of the randomness, which is the heart of the VAE magic.\n1. Verification of Your Intuition Point 1: Do 10 images of digit \u0026lsquo;1\u0026rsquo; cluster together?\nYes, you are right. The encoder will map all distinct images of \u0026lsquo;1\u0026rsquo; to values ofÂ $\\mu$Â andÂ $\\sigma$Â that are close to each other in the latent space. Nuance:Â They won\u0026rsquo;t beÂ exactlyÂ the same. One \u0026lsquo;1\u0026rsquo; might be slanted (mapping to slightly left in the cluster), and another might be bold (mapping to slightly right). The VAE captures these stylistic differences in the precise values ofÂ $\\mu$. Point 2: Is the latent codeÂ $z$Â stochastic for the same image?\nYes, you are right. If you feed the exact same image into a VAE multiple times (and you are using the sampling step), you will get a slightly different vectorÂ $z$Â each time because of the random noiseÂ $\\epsilon$. Note:Â In practical deployment (e.g., if using a VAE just to compress data), engineers often skip the sampling and just useÂ $\\mu$Â to get a deterministic code. But strictly speaking, the mathematical definition of the VAE inference path involves this randomness. 2. The Core Question: What is the use of the randomness? You asked:Â \u0026ldquo;For auto-encoder, the latent code is deterministic. What is the use of the randomness?\u0026rdquo;\nThis is the most critical concept in VAEs. The randomness transforms the latent space from aÂ discrete lookup table into aÂ continuous landscape.\nHere is the analogy:Â The Dot vs. The Bubble.\nA. The Standard Autoencoder (The Dot) Mechanism:Â It maps an input image to a single, precise point (a dot) in space. The Problem:Â The decoder only learns to decodeÂ that specific point. The Consequence:Â If you sample a point just slightly next to that dotâ€”in the \u0026ldquo;empty space\u0026rdquo; between a \u0026lsquo;1\u0026rsquo; and a \u0026lsquo;2\u0026rsquo;â€”the decoder has no idea what to do. It often produces garbage or static because it never learned to handle that specific coordinate during training. The latent space is full of \u0026ldquo;holes.\u0026rdquo; B. The VAE (The Bubble) Mechanism:Â By predicting a mean ($\\mu$) and variance ($\\sigma$) and adding noise, the encoder maps the input image not to a dot, but to aÂ cloudÂ orÂ bubbleÂ of probability. The Effect:Â During training, the decoder is forced to reconstruct the digit \u0026lsquo;1\u0026rsquo; not just from a single pointÂ $\\mu$, but fromÂ anyÂ point sampled within that bubble ($z$). The \u0026ldquo;Use\u0026rdquo; of Randomness: Forcing Continuity (Smoothness):Â Because the decoder must reconstruct a \u0026lsquo;1\u0026rsquo; from anywhere inside the bubble, it learns that pointsÂ nearÂ each other should produce similar outputs. This eliminates the \u0026ldquo;holes.\u0026rdquo; If two bubbles (say, a \u0026lsquo;1\u0026rsquo; and a \u0026lsquo;7\u0026rsquo;) overlap slightly, the decoder learns to generate a hybrid digit in that overlapping region. Dense Packing:Â The KL divergence (the \u0026ldquo;spring\u0026rdquo; we discussed earlier) tries to pack these bubbles as close to the center as possible without crushing them. Because they are bubbles (taking up volume) and not dots (infinitely small), they fill up the latent space completely. Short Answer The randomness prevents the model from \u0026ldquo;memorizing\u0026rdquo; specific points. It forces the model to learn aÂ regionÂ for each digit, ensuring that the latent space is smooth, continuous, and safe to sample from for generation.\nAppendix Derivation of the Gaussian Loss Here is the derivation of why the loss function looks the way it does, starting from the definition of the Gaussian distribution.\n1. The Gaussian Probability Density Function (PDF)\nFor a single pixel value $x$, modeled by a Gaussian with mean $\\mu$ and standard deviation $\\sigma$:\n$$ P(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} $$2. Log-Likelihood\nTo train models, we want to maximize the probability of the true data. We take the Logarithm to make the math easier (Log is monotonic, so maximizing Log($P$) is the same as maximizing $P$).\n$$ \\log P(x) = \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\log\\left(e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\\right) $$Using logarithm rules $\\log(e^y) = y$ and $\\log(1/a) = -\\log(a)$):\n$$ \\log P(x) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x - \\mu)^2}{2\\sigma^2} $$3. Negative Log-Likelihood (The Loss)\nIn Deep Learning, we minimize Loss, which is the Negative Log-Likelihood. We flip the signs:\n$$ \\text{Loss} = \\underbrace{\\frac{1}{2}\\log(2\\pi)}_{\\text{Constant}} + \\underbrace{\\log(\\sigma)}_{\\text{Variance Term}} + \\underbrace{\\frac{(x - \\mu)^2}{2\\sigma^2}}_{\\text{Error Term}} $$4. Simplified Loss for Analysis\nIgnoring the constant (since it doesn\u0026rsquo;t change with weights), we get the equation used in the \u0026ldquo;Cheating\u0026rdquo; section:\n$$ \\text{Loss} \\propto \\frac{(x - \\mu)^2}{2\\sigma^2} + \\log(\\sigma) $$Derivation of the ELBO (Evidence Lower Bound) The lecture did a great job in explaining ELBO. [1:03:04 to 1:08:55]\n$$ \\log p_\\theta(x) = \\log \\frac{p_\\theta(x|z) p(z)}{p_\\theta(z|x)} $$Multiply top and bottom by $\\textcolor{lightblue}{q_\\phi(z|x)}$\n$$ \\log p_\\theta(x) = \\log \\frac{p_\\theta(x|z)p(z)}{p_\\theta(z|x)} = \\log \\frac{p_\\theta(x|z)p(z)\\textcolor{lightblue}{q_\\phi(z|x)}}{p_\\theta(z|x)\\textcolor{lightblue}{q_\\phi(z|x)}} $$Logarithms + rearranging:\n$$ \\begin{align*} \\log p_\\theta(x) \u0026= \\log \\frac{p_\\theta(x \\mid z)p(z)}{p_\\theta(z \\mid x)} = \\log \\frac{\\textcolor{cyan}{p_\\theta(x \\mid z)}\\textcolor{green}{p(z)}\\textcolor{red}{q_\\phi(z \\mid x)}}{\\textcolor{yellow}{p_\\theta(z \\mid x)}\\textcolor{lightblue}{q_\\phi(z \\mid x)}} \\\\[12pt] \u0026= \\log \\textcolor{cyan}{p_\\theta(x \\mid z)} - \\log \\frac{\\textcolor{lightblue}{q_\\phi(z \\mid x)}}{\\textcolor{green}{p(z)}} + \\log \\frac{\\textcolor{red}{q_\\phi(z \\mid x)}}{\\textcolor{yellow}{p_\\theta(z \\mid x)}} \\end{align*} $$[1:04:01 - 1:04:58] We can wrap in an expectation since it doesnâ€™t depend on $z$: $\\log p_\\theta(x) = E_{z \\sim q_\\phi(z \\mid x)}\\left[\\log p_\\theta(x)\\right]$. The expectation (average) of a constant value is just the constant itself.\nIn the notationÂ $E_{z \\sim q_\\phi(z \\mid x)}\\left[\\log p_\\theta(x)\\right]$: $z$Â is indeed sampled from the distributionÂ $q_\\phi$. It is conditioned on the inputÂ $x$Â (\u0026ldquo;givenÂ x\u0026rdquo;). $q_\\phi(z \\mid x)$Â is theÂ Encoder. In plain English:Â \u0026ldquo;We are going to calculate the average value of [whatever term follows] by trying out many different latent codes ($z$) that our Encoder thinks are likely for this specific image ($x$).\u0026rdquo; The term they are looking at isÂ logP(x). This represents the probability of the image (data) occurring. The variableÂ $z$Â represents the latent code (hidden features). Crucially:Â $\\log P(x)$Â dependsÂ onlyÂ on the dataÂ $x$. It doesÂ notÂ depend onÂ $z$. Therefore, relative toÂ $z$, the termÂ $\\log P(x)$Â is aÂ constant. $$ \\log p_\\theta(x) = E_z[\\log p_\\theta(x \\mid z)] - E_z \\left[ \\log \\frac{q_\\phi(z \\mid x)}{p(z)} \\right] + E_z \\left[ \\log \\frac{q_\\phi(z \\mid x)}{p_\\theta(z \\mid x)} \\right] $$The 2nd and 3rd term are KL divergence which measures dissimilarity between two probability distributions.\n$$ \\begin{align*} \\log p_{\\theta}(x) \u0026= \\log \\frac{p_{\\theta}(x \\mid z)p(z)}{p_{\\theta}(z \\mid x)} = \\log \\frac{p_{\\theta}(x \\mid z)p(z)q_{\\phi}(z \\mid x)}{p_{\\theta}(z \\mid x)q_{\\phi}(z \\mid x)} \\\\ \u0026= E_{z}\\left[\\log p_{\\theta}(x \\mid z)\\right] - E_{z}\\left[\\log \\frac{q_{\\phi}(z \\mid x)}{p(z)}\\right] + E_{z}\\left[\\log \\frac{q_{\\phi}(z \\mid x)}{p_{\\theta}(z \\mid x)}\\right] \\\\ \u0026= E_{z \\sim q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] - D_{KL}\\left(q_{\\phi}(z \\mid x), p(z)\\right) + D_{KL}\\left(q_{\\phi}(z \\mid x), p_{\\theta}(z \\mid x)\\right) \\end{align*} $$$E_{z \\sim q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right]$: The data reconstruction term. xâ†’encoderâ†’decoder should reconstruct $x$. Can compute in closed form for Gaussians.\n$D_{KL}\\left(q_{\\phi}(z \\mid x), p(z)\\right)$: The prior term. We are forcing the Encoder to organize its output (i.e. the hidden codes $z$) so that, overall, they form a nice, neat Standard Gaussian distribution. Can compute in closed form for Gaussians.\nThe \u0026ldquo;Encoder Output\u0026rdquo;: This isÂ $q_\\phi(z \\mid x)$. When you feed an imageÂ $x$Â into the encoder, it doesn\u0026rsquo;t just give you a single point; it gives you a probability distribution (specifically, it predicts a meanÂ $\\mu$Â and a varianceÂ $\\sigma^2$Â for that image). The \u0026ldquo;Prior\u0026rdquo;:Â This isÂ $p(z)$. We assume this is aÂ Standard Unit GaussianÂ (a bell curve centered at 0 with a width of 1). The \u0026ldquo;Match\u0026rdquo;:Â The goal of training is to minimize the difference (KL Divergence) between the two. $D_{KL}\\left(q_{\\phi}(z \\mid x), p_{\\theta}(z \\mid x)\\right)$: Posterior Approximation. Encoder output $q_\\phi(z\\mid x)$ should match $p_\\theta(z\\mid x)$. We cannot compute this for Gaussians.\nKL is â‰¥ 0, so we can drop it to get lower bound on likelihood. This is out VAE training objective. Jointly train encoder $q$ and decoder $p$ to maximize the variational lower bound on the data likelihood. Also called Evidence Lower Bound (ELBo)\n$$ \\log p_{\\theta}(x) \\geq E_{z \\sim q_{\\phi}(z|x)} \\left[ \\log p_{\\theta}(x|z) \\right] - D_{KL} \\left( q_{\\phi}(z|x), p(z) \\right) $$\n","permalink":"http://localhost:1313/posts/vae-variational-auto-encoder/","summary":"\u003cp\u003eSource: \u003cstrong\u003eStanford CS231N Deep Learning for Computer Vision | Spring 2025 | Lecture 13: Generative Models 1 (\u003ca href=\"https://www.youtube.com/watch?v=zbHXQRUNlH0\u0026amp;list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16\u0026amp;index=13\u0026amp;t=3005s\"\u003elink\u003c/a\u003e)\u003c/strong\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"implementation\"\u003eImplementation\u003c/h1\u003e\n\u003ch2 id=\"pseudo-code\"\u003ePseudo Code\u003c/h2\u003e\n\u003ch3 id=\"encoder\"\u003e\u003cstrong\u003eEncoder\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe input image dimension is [$B$, $C$, $H$, $W$] in this case is [1, 28, 28], where 28x28 is the image size of MNIST data.\u003c/li\u003e\n\u003cli\u003eThe original classification head (\u003ccode\u003efc\u003c/code\u003e) is replaced to produce the mean (mu) and log-variance (\u003ccode\u003elogvar\u003c/code\u003e) required for the VAE\u0026rsquo;s reparameterization trick.\u003c/li\u003e\n\u003cli\u003eThe flow of encoder:\u003ccode\u003eImage â†’ Modified ResNet â†’ Project â†’ Chunk â†’\u003c/code\u003e \u003ccode\u003emu\u003c/code\u003e, \u003ccode\u003elogvar\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003e$[1, 28, 28]$ $\\xrightarrow{\\displaystyle \\textsf{ResNet}}$ [1, 1024] $\\xrightarrow{\\displaystyle \\textsf{Project}}$ $[1, 256]$ $\\xrightarrow{\\displaystyle \\textsf{Chunk}}$ $([1, 128], [1, 128])$\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"decoder\"\u003e\u003cstrong\u003eDecoder\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe flow of decoder: \u003ccode\u003eProject -\u0026gt; Reshape -\u0026gt; Nx Conv+Up\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eThe input to the decoder is the latent code $z$. The latent dimension is [$B$, $D_{in}$] ($D_{in}$ default to 128)\n\u003col\u003e\n\u003cli\u003eFirst it passes through a MLP â†’ [$B$, $D_{out}$]\u003c/li\u003e\n\u003cli\u003eReshape from [$B$, $D_{out}$] â†’ $[B, C_{init}, H_{init}, W_{init}]$, where $H_{init}=W_{init}$\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eThen it passes through N layers of Convolution + Up-sampleing layer â†’ $[B, 1, H_{out}, W_{out}]$, where $H_{out}=W_{out}=28$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"vae\"\u003e\u003cstrong\u003eVAE\u003c/strong\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe VAE takes data from MNIST dataset then pass it through a ResNet encoder.\u003c/li\u003e\n\u003cli\u003eThen sample latent code $z$ using $z = \\mu +\\sigma\\cdot\\epsilon$ where $\\epsilon \\sim N(0, I)$.\u003c/li\u003e\n\u003cli\u003eThen pass $z$ to the decoder.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"training\"\u003e\u003cstrong\u003eTraining\u003c/strong\u003e\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eRun input data through encoder to get distribution over $z$.\u003c/li\u003e\n\u003cli\u003ePrior loss: Encoder output should be unit Gaussian (zero mean, unit variance).\u003c/li\u003e\n\u003cli\u003eSample $z$ from encoder output $q_\\phi(z\\mid x)$ (Reparameterization trick).\u003c/li\u003e\n\u003cli\u003eRun $z$ through decoder to get predicted data mean.\u003c/li\u003e\n\u003cli\u003eReconstruction loss: predicted mean should match $x$ in L2.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"elaboration\"\u003eElaboration\u003c/h2\u003e\n\u003ch3 id=\"1-the-core-concept\"\u003e1. The Core Concept\u003c/h3\u003e\n\u003cp\u003eUnlike standard Autoencoders which mapÂ \u003ccode\u003eImage\u003c/code\u003eÂ $\\to$Â \u003ccode\u003eCode\u003c/code\u003eÂ $\\to$Â \u003ccode\u003eImage\u003c/code\u003e, a VAE mapsÂ \u003cstrong\u003e\u003ccode\u003eImage\u003c/code\u003eÂ $\\to$Â \u003ccode\u003eDistribution Parameters\u003c/code\u003eÂ $\\to$Â \u003ccode\u003eImage\u003c/code\u003e\u003c/strong\u003e.\u003c/p\u003e","title":"VAE (Variational Auto Encoder)"}]