<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>VAE Revisited 2026: The Foundation of Generative AI | Yechao&#39;s Blog</title>
<meta name="keywords" content="generative">
<meta name="description" content="VAEs are an especially important model to study if you want to understand modern generative modeling for two reasons. First, they introduced a clean probabilistic latent-variable framework for generation—showing how to learn a distribution over hidden representations and sample from it in a principled way. Second, VAEs remain central to state-of-the-art generative systems today: in many diffusion-based models (notably latent diffusion and Stable Diffusion [1]), a VAE is the component that compresses images into a latent space and decodes generated latents back into images, making high-quality generation practical and efficient.">
<meta name="author" content="">
<link rel="canonical" href="https://bai-yunhan.github.io/posts/vae-variational-auto-encoder/">
<meta name="google-site-verification" content="RskRZT3ZM_y-1j7BGujVZvbmoifgau1iF8jqSskz5a4">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://bai-yunhan.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://bai-yunhan.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://bai-yunhan.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://bai-yunhan.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://bai-yunhan.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://bai-yunhan.github.io/posts/vae-variational-auto-encoder/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"></script>

<script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "\\[", right: "\\]", display: true },
          { left: "\\(", right: "\\)", display: false },
          { left: "$", right: "$", display: false }
        ]
      });
    });
    </script>


<meta property="og:url" content="https://bai-yunhan.github.io/posts/vae-variational-auto-encoder/">
  <meta property="og:site_name" content="Yechao&#39;s Blog">
  <meta property="og:title" content="VAE Revisited 2026: The Foundation of Generative AI">
  <meta property="og:description" content="VAEs are an especially important model to study if you want to understand modern generative modeling for two reasons. First, they introduced a clean probabilistic latent-variable framework for generation—showing how to learn a distribution over hidden representations and sample from it in a principled way. Second, VAEs remain central to state-of-the-art generative systems today: in many diffusion-based models (notably latent diffusion and Stable Diffusion [1]), a VAE is the component that compresses images into a latent space and decodes generated latents back into images, making high-quality generation practical and efficient.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2026-01-27T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-27T00:00:00+00:00">
    <meta property="article:tag" content="Generative">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VAE Revisited 2026: The Foundation of Generative AI">
<meta name="twitter:description" content="VAEs are an especially important model to study if you want to understand modern generative modeling for two reasons. First, they introduced a clean probabilistic latent-variable framework for generation—showing how to learn a distribution over hidden representations and sample from it in a principled way. Second, VAEs remain central to state-of-the-art generative systems today: in many diffusion-based models (notably latent diffusion and Stable Diffusion [1]), a VAE is the component that compresses images into a latent space and decodes generated latents back into images, making high-quality generation practical and efficient.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://bai-yunhan.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "VAE Revisited 2026: The Foundation of Generative AI",
      "item": "https://bai-yunhan.github.io/posts/vae-variational-auto-encoder/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "VAE Revisited 2026: The Foundation of Generative AI",
  "name": "VAE Revisited 2026: The Foundation of Generative AI",
  "description": "VAEs are an especially important model to study if you want to understand modern generative modeling for two reasons. First, they introduced a clean probabilistic latent-variable framework for generation—showing how to learn a distribution over hidden representations and sample from it in a principled way. Second, VAEs remain central to state-of-the-art generative systems today: in many diffusion-based models (notably latent diffusion and Stable Diffusion [1]), a VAE is the component that compresses images into a latent space and decodes generated latents back into images, making high-quality generation practical and efficient.\n",
  "keywords": [
    "generative"
  ],
  "articleBody": "VAEs are an especially important model to study if you want to understand modern generative modeling for two reasons. First, they introduced a clean probabilistic latent-variable framework for generation—showing how to learn a distribution over hidden representations and sample from it in a principled way. Second, VAEs remain central to state-of-the-art generative systems today: in many diffusion-based models (notably latent diffusion and Stable Diffusion [1]), a VAE is the component that compresses images into a latent space and decodes generated latents back into images, making high-quality generation practical and efficient.\nIn this blog, I have two goals: (1) to build intuition for variational autoencoders (VAEs), and (2) to lay out the cleanest possible pseudo-code for probabilistic generation. I present the material in a top-down way—starting with what a VAE is and how it works, then diving into why it works. The mathematical perspective is largely drawn from Stanford CS231N: Deep Learning for Computer Vision (Spring 2025), Lecture 13: Generative Models 1 [2]. Alongside the post, I provide a corresponding GitHub repository with the actual training and inference code to make the ideas concrete.\nImplementation Pseudo Code Encoder The input image dimension is [$B$, $C$, $H$, $W$]. In this project, we use the MNIST dataset, where each image is grayscale and has spatial resolution $28\\times28$. A modified ResNet-18 [3], pre-trained on ImageNet, is used as the encoder. The original classification head (fc) is replaced to produce the mean (mu) and log-variance (logvar) required for the VAE’s reparameterization trick. The flow of encoder:Image → Modified ResNet → Project → Chunk → mu, logvar $[1, 28, 28]$ $\\xrightarrow{\\displaystyle \\textsf{ResNet}}$ [1, 1024] $\\xrightarrow{\\displaystyle \\textsf{Project}}$ $[1, 256]$ $\\xrightarrow{\\displaystyle \\textsf{Chunk}}$ $([1, 128], [1, 128])$ Decoder The decoder consists of one MLP layer followed by several convolutional layers (Conv) with upsampling (Up). The flow of decoder: Project -\u003e Reshape -\u003e Nx Conv+Up The input to the decoder is the latent code $z$. The latent dimension is [$B$, $D_{in}$] ($D_{in}$ default to 128) First it passes through a MLP → [$B$, $D_{out}$] Reshape from [$B$, $D_{out}$] → $[B, C_{init}, H_{init}, W_{init}]$, where $H_{init}=W_{init}$ Then it passes through $N$ layers of Convolution + Up-sampling layer → $[B, 1, H_{out}, W_{out}]$, where $H_{out}=W_{out}=28$ VAE The VAE takes data from MNIST dataset then pass it through a ResNet encoder. The encoder outputs the parameters $\\mu$ and $\\sigma$. Then sample latent code $z$ using $z = \\mu +\\sigma\\cdot\\epsilon$ where $\\epsilon \\sim N(0, I)$. Then pass $z$ to the decoder. Training Run input data $x$ through encoder to get distribution over $z$. Use prior loss to enforce the encoder outputs to follow a unit Gaussian distribution (zero mean, unit variance). Sample $z$ from encoder output $q_\\phi(z\\mid x)$ (Reparameterization trick). Run $z$ through decoder to get predicted data mean (Reconstruction). Use reconstruction loss to make predicted mean match $x$ under an L2 objective. Elaboration 1. The Core Concept Unlike standard Autoencoders which map Image $\\to$ Code $\\to$ Image, a VAE maps Image $\\to$ Distribution Parameters $\\to$ Image.\nThe networks do not output probabilities directly; they output the parameters (Mean $\\mu$ and Standard Deviation $\\sigma$) of Gaussian distributions.\n2. The Encoder (Inference Model) The encoder compresses high-dimensional data into a low-dimensional latent space.\nInput: Image of shape $3 \\times H \\times W$.\nOutput: Two vectors, both of length $D$ (the latent dimension, e.g., 128).\nMean Vector ($\\mu_z$): The center of the latent distribution. Log-Variance Vector ($\\log \\sigma^2_z$): The spread of the distribution. Design Choice of using Diagonal Covariance:\nWe assume the dimensions of $z$ are statistically independent. Instead of predicting a full $D \\times D$ covariance matrix (which would handle correlations between features), we only predict the diagonal.\nIntuition: This drastically reduces parameters from quadratic ($D^2$) to linear ($D$), making the model easier to train.\n3. The Reparameterization Trick The reparameterization trick is a tactical solution to a technical problem: How do we backpropagate through a random node?\nThe Issue: Inside the network, we need to sample $z$ from the distribution $q_\\phi(z|x)$ (typically a Gaussian with mean $\\mu$ and variance $\\sigma^2$. Standard random sampling breaks the chain of derivatives needed for backpropagation. The Trick: We move the randomness to an external variable $\\epsilon$ that is independent of the model parameters. The Equation: Instead of sampling $z \\sim N(\\mu, \\sigma^2)$ directly, we calculate: $z = \\mu + \\sigma \\odot \\epsilon$ where $\\epsilon \\sim N(0, 1)$ (standard normal distribution). This allows gradients to flow through $\\mu$ and $\\sigma$ during training, making the VAE end-to-end differentiable.\n4. The Decoder (Generative Model) The decoder reconstructs the image from a sampled latent point.\nInput: A vector $z$ of length $D$ (sampled from the Encoder’s distribution).\nOutput: A tensor of shape $3 \\times H \\times W$ (same as input image).\nWhat this output represents:\nMathematically, this output is the Mean Vector ($\\mu_x$) of the pixel probability distribution.\nDesign Choice (Fixed Variance):\nWe assume the pixel distribution is a Gaussian with a fixed standard deviation of 1 ($\\sigma=1$) and spherical covariance (no correlations between pixels).\n5. Critical Intuition: Why Design it This Way? A. Why Diagonal/Fixed Covariance? (The “Unmanageable Size” Problem)\nIf the Decoder tried to learn the correlations between every pair of pixels (Full Covariance), it would need a matrix of size $(H \\cdot W)^2$. For a small image, this is millions of parameters; for large images, trillions. Solution: By assuming pixels are independent (Diagonal) or fixed (Spherical), we reduce complexity from Quadratic to Linear. B. Why NOT Predict Separate Variances for Each Pixel? (The “Cheating” Problem)\nThe Idea: Why not let the decoder output a specific variance $\\sigma_i$ for every pixel $i$?\nThe Cheating Mechanism:\nAs derived in the Appendix, the Loss function includes two competing terms:\n$$ \\text{Loss} \\approx \\underbrace{\\frac{(x - \\mu)^2}{2\\sigma^2}}_{\\text{Reconstruction Error}} + \\underbrace{\\log(\\sigma)}_{\\text{Uncertainty Penalty}} $$ How it cheats: For difficult details (like edges), the reconstruction error $(x-\\mu)^2$ is naturally high. To minimize the Loss, the model can simply predict a massive variance ($\\sigma \\to \\infty$). The Result: A huge $\\sigma$ crushes the Reconstruction Error term to near zero. It is “cheaper” for the model to admit total uncertainty than to learn the hard feature. The Fix: By forcing $\\sigma=1$, the denominator is constant. The model must minimize $(x - \\mu)^2$ to lower the loss.\nC. Why is the “Reconstructed Image” just the Mean? (The MSE Connection)\nWhen you maximize the likelihood of a Gaussian where $\\sigma$ is fixed to 1, the math simplifies to:\n$\\text{Maximize } \\log P(x|z) \\iff \\text{Minimize } (x - \\mu)^2$\nTherefore, the output $\\mu$ is the value that minimizes the Mean Squared Error (L2 Loss).\nMotivation of VAE While standard auto-encoders are effective for learning feature representations (via reconstruction), they fail as generative models because their latent space ($Z$) has no enforced structure. Starting at [51:18].\nThe Limitation of Autoencoders [51:18]: if you want to use a standard autoencoder to generate new data, you would need to throw away the encoder and sample a latent vector $Z$ to pass through the decoder. However, because the autoencoder places no constraints on the latent space, you have no idea what the distribution of valid $Z$ vectors looks like. “Kicking the Can Down the Road” [51:57]: just as we didn’t know the distribution of the original data $X$, we now don’t know the distribution of the latent vectors $Z$. Therefore, we are “stuck” because we cannot easily sample a valid code to generate a realistic image. The VAE Solution [52:14]: The motivation for the VAE is to “force some structure on the $Z$’s.” By forcing the latent space to approximate a known distribution (typically a unit Gaussian), the VAE ensures that we can easily sample a random $Z$ from that known distribution and pass it through the decoder to generate valid new data. VAEs are essentially a “probabilistic spin” on traditional autoencoders designed specifically to enable this sampling capability [53:08].\nFormulation Goal — Maximize Marginal Likelihood We want to find parameters $\\theta$ such that the data we observed ($x$) is highly probable under our model. We maximize the Marginal Likelihood, not the Conditional Likelihood.\nEquation: $\\theta^* = \\operatorname*{argmax}_\\theta \\sum_{i} \\log p_\\theta(x^{(i)})$\nWhy Marginal $p_\\theta(x)$: The “marginal” integrates over all possible latent variables $z$.\n$p(x\\mid z)$ (Single Case): This asks, “How likely is this image if the hidden concept is exactly $z$?” $p(x)$ (Collectively all cases): This asks, “How likely is this image considering every possible hidden concept that could produce it?” i.e. $p(x) = \\int p(x|z)p(z)dz$. Symbol Name Description $p_\\theta(x \\mid z)$ Likelihood How likely is the image $x$, given the hidden traits $z$? $p_\\theta(z)$ Prior What is the distribution of hidden traits $z$ before seeing data? $p_\\theta(z \\mid x)$ Posterior Given the image $x$, what are the likely traits $z$? $p_\\theta(x)$ Marginal Likelihood How probable is the data $x$ overall, summing over all possible $z$’s? Why $p_\\theta(x)$ is called marginal likelihood?\nHistorically, probability distributions were written in tables. For example, if you had a joint distribution over $x$ and $z$:\nz=0 z=1 Row total x=0 … … p(x=0) x=1 … … p(x=1) Column total p(z=0) p(z=1) 1 To get the probability of $x$ alone, you would sum across the row, and the result would appear in the margin of the table. Hence: Summing out a variable = looking at the margin of the joint distribution. That operation was literally written in the margins of the table → “marginal probability.” The process of computing it → “marginalization.”\nBlocker ← The Intractability To calculate the marginal likelihood, we face a mathematical dead end because we cannot compute the integral or the posterior.\nThe Integral Problem: $p_\\theta(x) = \\int p_\\theta(x|z)p(z) \\, dz$\nFor complex data (like images), this integral is impossible to calculate (intractable) because it requires summing over an infinite number of possible $z$ configurations.\nThe Bayes’ Rule Problem:\nWe might try to find $p_\\theta(x)$ via Bayes’ Rule:\n$$ p_\\theta(x) = \\frac{p_\\theta(x\\mid z)p(z)}{p_\\theta(z\\mid x)} $$However, we only have a decoder to compute $p_\\theta(x\\mid z)$. To compute the denominator $p_\\theta(z|x)$ (the true posterior), we need to use the Bayes’ rule which requires knowing $p_\\theta(x)$, $p_\\theta(z\\mid x) = \\frac{p_\\theta(x\\mid z)p(z)}{p_\\theta(x)}$, so it is intractable.\nSolution ← Variational Inference (Replacing $p_\\theta$ with $q_\\phi$) Since the true posterior $p_\\theta(z|x)$ is impossible to calculate, we approximate it with a tractable distribution $q_\\phi(z|x)$ (the Encoder/Neural Network).\nThe Approximation: $q_\\phi(z|x) \\approx p_\\theta(z|x)$ Intuition The reconstruction loss and prior loss is fighting with each other This “tug-of-war” is one of the most fascinating parts of a VAE because you can actually see the result of these two forces fighting when you plot the latent space.\nHere is the visual evidence that the KL divergence doesn’t just “force” everything to zero, but rather organizes it.\n1. The “Healthy” VAE Balance When the Reconstruction Loss (keep data distinct) and KL Loss (keep data Gaussian) are balanced correctly, the latent space looks like this:\nWhat to notice:\nGlobal Centering: Notice that the entire cloud of points is centered around $(0,0)$ and roughly spans between $-3$ and $3$ (typical for a unit variance). This is the KL loss doing its job. Local Distinctness: Crucially, individual digits are not all collapsed to $(0,0)$. The “7\"s might be clustered at $(-1, 2)$ and the “0\"s at $(1, -1)$. The encoder has learned to shift the mean $\\mu$ away from 0 just enough to distinguish the digits, but keeps them packed tight enough to satisfy the Gaussian prior. Smoothness: The clusters touch each other. This means if you sample a point halfway between a “1” and a “7”, you get a digit that looks like a plausible mix of both. 2. What happens without the KL “Force” (Standard Autoencoder) If you remove the KL term (essentially predicting mean/variance but with no penalty for where they are), the Reconstruction Loss takes over completely.\nWhat to notice:\nGaps and Explosions: The clusters fly apart. The model might put “0\"s at $(100, 100)$ and “1\"s at $(-50, -50)$ just to be absolutely sure it doesn’t confuse them. No Structure: There is no center. The spread is arbitrary. Dead Zones: There are massive empty gaps between clusters. If you try to generate an image from those gaps, you get static/noise because the decoder has never seen data there. 3. What happens if the KL Force Wins (Posterior Collapse) This is the scenario you feared—where the model is “forced” to 0 and 1.\nThe Visual: Imagine the first plot, but all the colored clusters are stacked directly on top of each other at $(0,0)$. The Result: The encoder output is always $\\mu=0, \\sigma=1$ effectively ignoring the input image. The decoder receives pure noise every time and produces a single, blurry “average” image (like a gray ghost) for every single input. 4. Summary The KL divergence acts like a spring attached to the origin $(0,0)$.\nReconstruction tries to pull the data points apart so they don’t overlap. KL Divergence (the spring) pulls them back toward the center. The final state is a tense equilibrium: data points distinct enough to be recognized, but bunched tight enough to form a smooth, continuous space. Role of $z_i$ in latent code $Z$ Auto-Encoding Variational Bayes, ICLR 2014 [4].\nIn a Variational Autoencoder (VAE), $Z$ represents the latent code—a compressed, hidden representation of the input data. $z_i$ refers to a specific individual dimension (or component) within this vector. The core idea shown here is disentanglement: the model attempts to map distinct, meaningful semantic features of the data to separate dimensions ($z_i$). For example, changing the value of one dimension ($z_1$) might smoothly transform the digit’s identity (e.g., changing a 6 to a 0), while changing another dimension ($z_2$) might strictly alter the slant or thickness of the writing, without changing the digit itself.\nImplementation of sampling $z_1$ and $z_2$ and set rest of $z_i$ to zeros.\nLimitation: Blurry results (i.e. What’s Next?) The nature of loss function (MSE/log-likelihood) tend to produce “blurry” results in VAE.\nThe “blurry VAE” phenomenon is a direct mathematical consequence of how we measure “error” using Mean Squared Error (MSE) or Log-Likelihood.\nIn short: MSE forces the model to be a conservative “average,” rather than a bold “guesser.”\nHere is the breakdown of why this happens.\n1. The “Safety” of the Average Imagine the model is trying to reconstruct a picture of a zebra, but the latent space is slightly uncertain about exactly where a specific stripe should be.\nPossibility A: The stripe is at pixel 100. Possibility B: The stripe is at pixel 101. If the model guesses A (sharp stripe at 100), but the truth was B, the MSE penalty is massive because the pixel values are totally opposite (black vs. white).\nIf the model guesses B (sharp stripe at 101), but the truth was A, the penalty is essentially double (wrong on both pixels).\nThe VAE’s Solution: It predicts a gray smear across pixels 100 and 101.\nWhy? Gray is never “perfectly right,” but it is never “catastrophically wrong.” It minimizes the squared error across all plausible possibilities. The model learns to hedge its bets to lower the loss. 2. The Unimodal Assumption (Mathematics) This is the technical root of the problem.\nMSE is equivalent to Maximum Likelihood under a Gaussian distribution. When you use MSE, you are implicitly telling the model: “Assume the pixel value comes from a single Bell curve (Unimodal).” The Problem: Real data is Multimodal.\nA pixel at the edge of an object could plausibly be Black (background) OR White (object). It is almost never Gray.\nMultimodal Reality: Two peaks (one at 0, one at 255). Unimodal Constraint: The model must fit one Bell curve to explain both peaks. Result: The model centers the Bell curve right in the middle (Gray/Blur) to cover both options. 3. High Frequency vs. Low Frequency Low Frequency (Structure): Where is the head? Where is the background? VAEs are great at this because the “average” of a head is still roughly a head shape. High Frequency (Texture/Edges): Where is this specific hair strand? Where is the pore on the skin? The “average” of many possible hair strand positions is just a smooth blur. Since VAEs optimize for the average case, they effectively apply a low-pass filter to the image, smoothing out all the sharp “high frequency” noise that our eyes interpret as realistic detail. Comparison: Why GANs [5] don’t blur VAE (MSE): “I must be close to the pixel values on average. I will be safe and blurry.” GAN (Discriminator): “I don’t care if the stripe is at pixel 100 or 101, but if it’s gray/blurry, the Discriminator will know it’s fake. I must pick one sharp location, even if I guess wrong.” Loss Function Strategy Result MSE / L2 Minimize variance; fit the mean of the distribution. Blurry. The mean of “sharp left” and “sharp right” is “blurry middle.” L1 Loss Minimize absolute error; fit the median. Slightly sharper, but still blurry compared to GANs. Adversarial Fool a judge; match the distribution. Sharp. Forces a decision (collapse to a mode) rather than an average. The Perceptual Loss (VGG Loss) [6] or VQ-VAE [7] are the standard modern techniques specifically designed to fix this blurriness in VAEs without needing a full GAN setup.\nRetrospection Question-1: In a VAE, does the KL term make each class (e.g., each digit 0–9) form its own Gaussian cluster in latent space near zero, such that the mixture of the 10 class-wise clusters matches a standard normal, and the class separability comes from different encoder-produced $\\mu$ and $\\sigma$?\nConcretely, We sample $z$ from the encoder, $z \\sim q_\\phi(z|x)$, where $q_\\phi(z|x)$ is a gaussian distribution, i.e. $z \\sim N(\\mu, \\sigma^2)$. We use KL divergence loss to make $N(\\mu, \\sigma^2)$ look similar to a standard gaussian distribution. $z$ is a latent code (vector of length $D$). Say, we are reconstructing hand written digit of 10 classes (digit 0 ~ 9). The $z$ representing digit $0$ is a gaussian distribution, and the $z$ representing digit $1$ is another gaussian distribution. These cluster both locate near to zero mean. The totality of the 10 cluster conform to the standard gaussian distribution. The distinctiveness between each cluster comes from the difference in $\\mu$ and $\\sigma$ generated by the encoder.\nAnswer: The understanding is correct.\nIn summary, the VAE is a Master of Compromise\nThe VAE does clustering (via the reconstruction term), where each class (digit) gets its own localized cluster defined by its mean $\\mu$. The VAE does regularization (via the KL term), ensuring that all these clusters stay packed together in a smooth, continuous space that collectively resembles the $N(\\mu, \\sigma^2)$ distribution. The Local View (Individual Input) For any single input (e.g., a specific image of a digit ‘2’), the encoder outputs a specific $\\mu$ and $\\sigma$. This defines a “neighborhood” in the latent space where that specific image lives. By sampling $z$ from this neighborhood, the decoder learns that any point in this small area should look like that specific ‘2’. The Class View (Group of Inputs belong to same class) As you noted, all the ‘2’s will have their own $\\mu$ and $\\sigma$ values. Because they all share similar visual features, the Reconstruction Loss naturally forces their $\\mu$ values to be near each other. This creates a cluster (a Gaussian Mixture component) for the digit ‘2’. The Global View (The Prior) The KL Divergence is the “global supervisor.” It doesn’t care about the labels (0–9); it only sees the totality of all these neighborhoods. It exerts a pull on every individual distribution to stay close to $0$ and have a spread near $1$. The result is that the “cloud of clusters” (the totality) conforms to the standard Gaussian shape. A Helpful Mental Model: “The Bubble Map” Imagine each input image is a bubble.\nThe Reconstruction Loss wants the bubbles to be solid and distinct so the decoder can “see” the image clearly. The KL Loss wants all the bubbles to move to the center of the map and be exactly the same size. The Training Process is the struggle to pack all these bubbles into a small, circular container (the Standard Normal Prior) without them overlapping so much that they lose their identity. Question-2: What is the use of the randomness in VAE ?\nConcretely, let’s say we are reconstructing hand written digit of 10 classes (digit 0 ~ 9).\nRegarding the encoder, when given 10 input images of digit ‘1’, the output $\\mu$ and $\\sigma$ would be very close if not the same. Namely, the latent code $z$ of the image of digit ‘1’ belong to the same cluster. Am i right?\nDuring inference, if we feed the same image of ‘1’ to the encoder, though the $\\mu$ and $\\sigma$ output by encoder is the same, due to the randomness of $\\epsilon$, the output $z$ would be different. namely, the latent code for the same input image is not deterministic. Am i right? In contradiction, for auto-encoder, the latent code for the same input image is deterministic. What is the use of the randomness?\nAnswer: Randomness is the heart of the VAE magic.\n1. Verification of Intuition\nPoint 1: Do 10 images of digit ‘1’ cluster together?\nYes, you are right. The encoder will map all distinct images of ‘1’ to values of $\\mu$ and $\\sigma$ that are close to each other in the latent space. Nuance: They won’t be exactly the same. One ‘1’ might be slanted (mapping to slightly left in the cluster), and another might be bold (mapping to slightly right). The VAE captures these stylistic differences in the precise values of $\\mu$. Point 2: Is the latent code $z$ stochastic for the same image?\nYes, you are right. If you feed the exact same image into a VAE multiple times (and you are using the sampling step), you will get a slightly different vector $z$ each time because of the random noise $\\epsilon$. Note: In practical deployment (e.g., if using a VAE just to compress data), engineers often skip the sampling and just use $\\mu$ to get a deterministic code. But strictly speaking, the mathematical definition of the VAE inference path involves this randomness. 2. The Core Question: What is the use of the randomness?\nYou asked: “For auto-encoder, the latent code is deterministic. What is the use of the randomness?” This is the most critical concept in VAEs. The randomness transforms the latent space from a discrete lookup table into a continuous landscape. Here is the analogy: The Dot vs. The Bubble.\nA. The Standard Autoencoder (The Dot)\nMechanism: It maps an input image to a single, precise point (a dot) in space. The Problem: The decoder only learns to decode that specific point. The Consequence: If you sample a point just slightly next to that dot—in the “empty space” between a ‘1’ and a ‘2’—the decoder has no idea what to do. It often produces garbage or static because it never learned to handle that specific coordinate during training. The latent space is full of “holes.” B. The VAE (The Bubble)\nMechanism: By predicting a mean $\\mu$ and variance $\\sigma$ and adding noise, the encoder maps the input image not to a dot, but to a cloud or bubble of probability. The Effect: During training, the decoder is forced to reconstruct the digit ‘1’ not just from a single point $\\mu$, but from any point sampled within that bubble $z$. The “Use” of Randomness: Forcing Continuity (Smoothness): Because the decoder must reconstruct a ‘1’ from anywhere inside the bubble, it learns that points near each other should produce similar outputs. This eliminates the “holes.” If two bubbles (say, a ‘1’ and a ‘7’) overlap slightly, the decoder learns to generate a hybrid digit in that overlapping region. Dense Packing: The KL divergence (the “spring” we discussed earlier) tries to pack these bubbles as close to the center as possible without crushing them. Because they are bubbles (taking up volume) and not dots (infinitely small), they fill up the latent space completely. The randomness prevents the model from “memorizing” specific points. It forces the model to learn a region for each digit, ensuring that the latent space is smooth, continuous, and safe to sample from for generation.\nAppendix Derivation of the Gaussian Loss Here is the derivation of why the loss function looks the way it does, starting from the definition of the Gaussian distribution.\n1. The Gaussian Probability Density Function (PDF)\nFor a single pixel value $x$, modeled by a Gaussian with mean $\\mu$ and standard deviation $\\sigma$:\n$$ P(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} $$2. Log-Likelihood\nTo train models, we want to maximize the probability of the true data. We take the Logarithm to make the math easier (Log is monotonic, so maximizing Log($P$) is the same as maximizing $P$).\n$$ \\log P(x) = \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\log\\left(e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\\right) $$Using logarithm rules $\\log(e^y) = y$ and $\\log(1/a) = -\\log(a)$):\n$$ \\log P(x) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x - \\mu)^2}{2\\sigma^2} $$3. Negative Log-Likelihood (The Loss)\nIn Deep Learning, we minimize Loss, which is the Negative Log-Likelihood. We flip the signs:\n$$ \\text{Loss} = \\underbrace{\\frac{1}{2}\\log(2\\pi)}_{\\text{Constant}} + \\underbrace{\\log(\\sigma)}_{\\text{Variance Term}} + \\underbrace{\\frac{(x - \\mu)^2}{2\\sigma^2}}_{\\text{Error Term}} $$4. Simplified Loss for Analysis\nIgnoring the constant (since it doesn’t change with weights), we get the equation used in the “Cheating” section:\n$$ \\text{Loss} \\propto \\frac{(x - \\mu)^2}{2\\sigma^2} + \\log(\\sigma) $$Derivation of the ELBO (Evidence Lower Bound) The lecture did a great job in explaining ELBO. [1:03:04 to 1:08:55]\n$$ \\log p_\\theta(x) = \\log \\frac{p_\\theta(x|z) p(z)}{p_\\theta(z|x)} $$Multiply top and bottom by $\\textcolor{lightblue}{q_\\phi(z|x)}$\n$$ \\log p_\\theta(x) = \\log \\frac{p_\\theta(x|z)p(z)}{p_\\theta(z|x)} = \\log \\frac{p_\\theta(x|z)p(z)\\textcolor{lightblue}{q_\\phi(z|x)}}{p_\\theta(z|x)\\textcolor{lightblue}{q_\\phi(z|x)}} $$Logarithms + rearranging:\n$$ \\begin{align*} \\log p_\\theta(x) \u0026= \\log \\frac{p_\\theta(x \\mid z)p(z)}{p_\\theta(z \\mid x)} = \\log \\frac{\\textcolor{cyan}{p_\\theta(x \\mid z)}\\textcolor{green}{p(z)}\\textcolor{red}{q_\\phi(z \\mid x)}}{\\textcolor{yellow}{p_\\theta(z \\mid x)}\\textcolor{lightblue}{q_\\phi(z \\mid x)}} \\\\[12pt] \u0026= \\log \\textcolor{cyan}{p_\\theta(x \\mid z)} - \\log \\frac{\\textcolor{lightblue}{q_\\phi(z \\mid x)}}{\\textcolor{green}{p(z)}} + \\log \\frac{\\textcolor{red}{q_\\phi(z \\mid x)}}{\\textcolor{yellow}{p_\\theta(z \\mid x)}} \\end{align*} $$[1:04:01 - 1:04:58] We can wrap in an expectation since it doesn’t depend on $z$: $\\log p_\\theta(x) = E_{z \\sim q_\\phi(z \\mid x)}\\left[\\log p_\\theta(x)\\right]$. The expectation (average) of a constant value is just the constant itself.\nIn the notation $E_{z \\sim q_\\phi(z \\mid x)}\\left[\\log p_\\theta(x)\\right]$: $z$ is indeed sampled from the distribution $q_\\phi$. It is conditioned on the input $x$ (“given x”). $q_\\phi(z \\mid x)$ is the Encoder. In plain English: “We are going to calculate the average value of [whatever term follows] by trying out many different latent codes ($z$) that our Encoder thinks are likely for this specific image ($x$).” The term they are looking at is logP(x). This represents the probability of the image (data) occurring. The variable $z$ represents the latent code (hidden features). Crucially: $\\log P(x)$ depends only on the data $x$. It does not depend on $z$. Therefore, relative to $z$, the term $\\log P(x)$ is a constant. $$ \\log p_\\theta(x) = E_z[\\log p_\\theta(x \\mid z)] - E_z \\left[ \\log \\frac{q_\\phi(z \\mid x)}{p(z)} \\right] + E_z \\left[ \\log \\frac{q_\\phi(z \\mid x)}{p_\\theta(z \\mid x)} \\right] $$The 2nd and 3rd term are KL divergence which measures dissimilarity between two probability distributions.\n$$ \\begin{align*} \\log p_{\\theta}(x) \u0026= \\log \\frac{p_{\\theta}(x \\mid z)p(z)}{p_{\\theta}(z \\mid x)} = \\log \\frac{p_{\\theta}(x \\mid z)p(z)q_{\\phi}(z \\mid x)}{p_{\\theta}(z \\mid x)q_{\\phi}(z \\mid x)} \\\\ \u0026= E_{z}\\left[\\log p_{\\theta}(x \\mid z)\\right] - E_{z}\\left[\\log \\frac{q_{\\phi}(z \\mid x)}{p(z)}\\right] + E_{z}\\left[\\log \\frac{q_{\\phi}(z \\mid x)}{p_{\\theta}(z \\mid x)}\\right] \\\\ \u0026= E_{z \\sim q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right] - D_{KL}\\left(q_{\\phi}(z \\mid x), p(z)\\right) + D_{KL}\\left(q_{\\phi}(z \\mid x), p_{\\theta}(z \\mid x)\\right) \\end{align*} $$$E_{z \\sim q_{\\phi}(z \\mid x)}\\left[\\log p_{\\theta}(x \\mid z)\\right]$: The data reconstruction term. x→encoder→decoder should reconstruct $x$. Can compute in closed form for Gaussians.\n$D_{KL}\\left(q_{\\phi}(z \\mid x), p(z)\\right)$: The prior term. We are forcing the Encoder to organize its output (i.e. the hidden codes $z$) so that, overall, they form a nice, neat Standard Gaussian distribution. Can compute in closed form for Gaussians.\nThe “Encoder Output”: This is $q_\\phi(z \\mid x)$. When you feed an image $x$ into the encoder, it doesn’t just give you a single point; it gives you a probability distribution (specifically, it predicts a mean $\\mu$ and a variance $\\sigma^2$ for that image). The “Prior”: This is $p(z)$. We assume this is a Standard Unit Gaussian (a bell curve centered at 0 with a width of 1). The “Match”: The goal of training is to minimize the difference (KL Divergence) between the two. $D_{KL}\\left(q_{\\phi}(z \\mid x), p_{\\theta}(z \\mid x)\\right)$: Posterior Approximation. Encoder output $q_\\phi(z\\mid x)$ should match $p_\\theta(z\\mid x)$. We cannot compute this for Gaussians.\nKL is ≥ 0, so we can drop it to get lower bound on likelihood. This is out VAE training objective. Jointly train encoder $q$ and decoder $p$ to maximize the variational lower bound on the data likelihood. Also called Evidence Lower Bound (ELBo)\n$$ \\log p_{\\theta}(x) \\geq E_{z \\sim q_{\\phi}(z|x)} \\left[ \\log p_{\\theta}(x|z) \\right] - D_{KL} \\left( q_{\\phi}(z|x), p(z) \\right) $$\nCitation Please cite this work as:\nBai, Yechao. \"VAE Revisited 2026: The Foundation of Generative AI\". Yechao's Log (Jan 2026). https://bai-yunhan.github.io/posts/vae-variational-auto-encoder Or use the BibTex citation:\n@article{Bai2026VAE, title = {VAE Revisited 2026: The Foundation of Generative AI}, author = {Bai, Yechao}, journal = {bai-yunhan.github.io}, year = {2026}, month = {Jan}, url = \"https://bai-yunhan.github.io/posts/vae-variational-auto-encoder/\" } References High-Resolution Image Synthesis with Latent Diffusion Models (Stable Diffusion)\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., \u0026 Ommer, B. (2022). Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\narXiv:2112.10752\nStanford CS231N: Deep Learning for Computer Vision (Lecture 13, Spring 2025)\nYouTube Link\nDeep Residual Learning for Image Recognition (ResNet)\nHe, K., Zhang, X., Ren, S., \u0026 Sun, J. (2016). Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\narXiv:1512.03385\nAuto-Encoding Variational Bayes (Original VAE Paper)\nKingma, D. P., \u0026 Welling, M. (2014). Auto-Encoding Variational Bayes. International Conference on Learning Representations (ICLR).\narXiv:1312.6114\nGenerative Adversarial Nets (GAN)\nGoodfellow, I., et al. (2014). Advances in Neural Information Processing Systems (NeurIPS).\narXiv:1406.2661\nPerceptual Losses for Real-Time Style Transfer and Super-Resolution\nJohnson, J., Alahi, A., \u0026 Fei-Fei, L. (2016). European Conference on Computer Vision (ECCV).\narXiv:1603.08155\nNeural Discrete Representation Learning (VQ-VAE)\nvan den Oord, A., Vinyals, O., \u0026 Kavukcuoglu, K. (2017). Advances in Neural Information Processing Systems (NeurIPS).\narXiv:1711.00937\n",
  "wordCount" : "4978",
  "inLanguage": "en",
  "datePublished": "2026-01-27T00:00:00Z",
  "dateModified": "2026-01-27T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://bai-yunhan.github.io/posts/vae-variational-auto-encoder/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Yechao's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://bai-yunhan.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://bai-yunhan.github.io/" accesskey="h" title="Yechao&#39;s Blog (Alt + H)">Yechao&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://bai-yunhan.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://bai-yunhan.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://bai-yunhan.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://bai-yunhan.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      VAE Revisited 2026: The Foundation of Generative AI
    </h1>
    <div class="post-meta"><span title='2026-01-27 00:00:00 +0000 UTC'>January 27, 2026</span>&nbsp;·&nbsp;<span>24 min</span>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#implementation" aria-label="Implementation">Implementation</a><ul>
                        
                <li>
                    <a href="#pseudo-code" aria-label="Pseudo Code">Pseudo Code</a><ul>
                        
                <li>
                    <a href="#encoder" aria-label="Encoder">Encoder</a></li>
                <li>
                    <a href="#decoder" aria-label="Decoder">Decoder</a></li>
                <li>
                    <a href="#vae" aria-label="VAE">VAE</a></li>
                <li>
                    <a href="#training" aria-label="Training">Training</a></li></ul>
                </li>
                <li>
                    <a href="#elaboration" aria-label="Elaboration">Elaboration</a><ul>
                        
                <li>
                    <a href="#1-the-core-concept" aria-label="1. The Core Concept">1. The Core Concept</a></li>
                <li>
                    <a href="#2-the-encoder-inference-model" aria-label="2. The Encoder (Inference Model)">2. The Encoder (Inference Model)</a></li>
                <li>
                    <a href="#3-the-reparameterization-trick" aria-label="3. The Reparameterization Trick">3. The Reparameterization Trick</a></li>
                <li>
                    <a href="#4-the-decoder-generative-model" aria-label="4. The Decoder (Generative Model)">4. The Decoder (Generative Model)</a></li>
                <li>
                    <a href="#5-critical-intuition-why-design-it-this-way" aria-label="5. Critical Intuition: Why Design it This Way?">5. Critical Intuition: Why Design it This Way?</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#motivation-of-vae" aria-label="Motivation of VAE">Motivation of VAE</a></li>
                <li>
                    <a href="#formulation" aria-label="Formulation">Formulation</a><ul>
                        
                <li>
                    <a href="#goal--maximize-marginal-likelihood" aria-label="Goal — Maximize Marginal Likelihood">Goal — Maximize Marginal Likelihood</a></li>
                <li>
                    <a href="#blocker--the-intractability" aria-label="Blocker ← The Intractability">Blocker ← The Intractability</a></li>
                <li>
                    <a href="#solution--variational-inference-replacingwith" aria-label="Solution ← Variational Inference (Replacing $p_\theta$ with $q_\phi$)">Solution ← Variational Inference (Replacing $p_\theta$ with $q_\phi$)</a></li></ul>
                </li>
                <li>
                    <a href="#intuition" aria-label="Intuition">Intuition</a><ul>
                        
                <li>
                    <a href="#the-reconstruction-loss-and-prior-loss-is-fighting-with-each-other" aria-label="The reconstruction loss and prior loss is fighting with each other">The reconstruction loss and prior loss is fighting with each other</a><ul>
                        
                <li>
                    <a href="#1-the-healthy-vae-balance" aria-label="1. The &ldquo;Healthy&rdquo; VAE Balance">1. The &ldquo;Healthy&rdquo; VAE Balance</a></li>
                <li>
                    <a href="#2-what-happenswithoutthe-kl-force-standard-autoencoder" aria-label="2. What happens without the KL &ldquo;Force&rdquo; (Standard Autoencoder)">2. What happens without the KL &ldquo;Force&rdquo; (Standard Autoencoder)</a></li>
                <li>
                    <a href="#3-what-happens-if-the-kl-force-wins-posterior-collapse" aria-label="3. What happens if the KL Force Wins (Posterior Collapse)">3. What happens if the KL Force Wins (Posterior Collapse)</a></li>
                <li>
                    <a href="#4-summary" aria-label="4. Summary">4. Summary</a></li></ul>
                </li>
                <li>
                    <a href="#role-of--in-latent-code" aria-label="Role of $z_i$ in latent code $Z$">Role of $z_i$ in latent code $Z$</a></li></ul>
                </li>
                <li>
                    <a href="#limitation-blurry-results-ie-whats-next" aria-label="Limitation: Blurry results (i.e. What’s Next?)">Limitation: Blurry results (i.e. What’s Next?)</a><ul>
                        <ul>
                        
                <li>
                    <a href="#1-the-safety-of-the-average" aria-label="1. The &ldquo;Safety&rdquo; of the Average">1. The &ldquo;Safety&rdquo; of the Average</a></li>
                <li>
                    <a href="#2-the-unimodal-assumption-mathematics" aria-label="2. The Unimodal Assumption (Mathematics)">2. The Unimodal Assumption (Mathematics)</a></li>
                <li>
                    <a href="#3-high-frequency-vs-low-frequency" aria-label="3. High Frequency vs. Low Frequency">3. High Frequency vs. Low Frequency</a></li>
                <li>
                    <a href="#comparison-why-gans-5-dont-blur" aria-label="Comparison: Why GANs [5] don&rsquo;t blur">Comparison: Why GANs [5] don&rsquo;t blur</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#retrospection" aria-label="Retrospection">Retrospection</a></li>
                <li>
                    <a href="#appendix" aria-label="Appendix">Appendix</a><ul>
                        
                <li>
                    <a href="#derivation-of-the-gaussian-loss" aria-label="Derivation of the Gaussian Loss">Derivation of the Gaussian Loss</a></li>
                <li>
                    <a href="#derivation-of-the-elbo-evidence-lower-bound" aria-label="Derivation of the ELBO (Evidence Lower Bound)">Derivation of the ELBO (Evidence Lower Bound)</a></li></ul>
                </li>
                <li>
                    <a href="#citation" aria-label="Citation">Citation</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>VAEs are an especially important model to study if you want to understand modern generative modeling for two reasons. First, they introduced a clean probabilistic latent-variable framework for generation—showing how to learn a distribution over hidden representations and sample from it in a principled way. Second, VAEs remain central to state-of-the-art generative systems today: in many diffusion-based models (notably latent diffusion and Stable Diffusion [1]), a VAE is the component that compresses images into a latent space and decodes generated latents back into images, making high-quality generation practical and efficient.</p>
<p>In this blog, I have two goals: (1) to build intuition for variational autoencoders (VAEs), and (2) to lay out the cleanest possible pseudo-code for probabilistic generation. I present the material in a top-down way—starting with what a VAE is and how it works, then diving into why it works. The mathematical perspective is largely drawn from <a href="https://www.youtube.com/watch?v=zbHXQRUNlH0">Stanford CS231N: Deep Learning for Computer Vision (Spring 2025), Lecture 13: Generative Models 1</a> [2]. Alongside the post, I provide a <a href="https://github.com/Bai-YunHan/miniVAE">corresponding GitHub repository</a> with the actual training and inference code to make the ideas concrete.</p>
<hr>
<h1 id="implementation">Implementation<a hidden class="anchor" aria-hidden="true" href="#implementation">#</a></h1>
<h2 id="pseudo-code">Pseudo Code<a hidden class="anchor" aria-hidden="true" href="#pseudo-code">#</a></h2>
<h3 id="encoder"><strong>Encoder</strong><a hidden class="anchor" aria-hidden="true" href="#encoder">#</a></h3>
<ul>
<li>The input image dimension is [$B$, $C$, $H$, $W$]. In this project, we use the MNIST dataset, where each image is grayscale and has spatial resolution $28\times28$.</li>
<li>A modified ResNet-18 [3], pre-trained on ImageNet, is used as the encoder. The original classification head (<code>fc</code>) is replaced to produce the mean (<code>mu</code>) and log-variance (<code>logvar</code>) required for the VAE&rsquo;s reparameterization trick.</li>
<li>The flow of encoder:<code>Image → Modified ResNet → Project → Chunk →</code> <code>mu</code>, <code>logvar</code>
<ul>
<li>$[1, 28, 28]$ $\xrightarrow{\displaystyle \textsf{ResNet}}$ [1, 1024] $\xrightarrow{\displaystyle \textsf{Project}}$ $[1, 256]$ $\xrightarrow{\displaystyle \textsf{Chunk}}$ $([1, 128], [1, 128])$</li>
</ul>
</li>
</ul>
<h3 id="decoder"><strong>Decoder</strong><a hidden class="anchor" aria-hidden="true" href="#decoder">#</a></h3>
<ul>
<li>The decoder consists of one <code>MLP</code> layer followed by several convolutional layers (<code>Conv</code>) with upsampling (<code>Up</code>).</li>
<li>The flow of decoder: <code>Project -&gt; Reshape -&gt; Nx Conv+Up</code></li>
<li>The input to the decoder is the latent code $z$. The latent dimension is [$B$, $D_{in}$] ($D_{in}$ default to 128)
<ol>
<li>First it passes through a <code>MLP</code> → [$B$, $D_{out}$]</li>
<li>Reshape from [$B$, $D_{out}$] → $[B, C_{init}, H_{init}, W_{init}]$, where $H_{init}=W_{init}$</li>
</ol>
</li>
<li>Then it passes through $N$ layers of Convolution + Up-sampling layer → $[B, 1, H_{out}, W_{out}]$, where $H_{out}=W_{out}=28$</li>
</ul>
<h3 id="vae"><strong>VAE</strong><a hidden class="anchor" aria-hidden="true" href="#vae">#</a></h3>
<ul>
<li>The VAE takes data from MNIST dataset then pass it through a ResNet encoder. The encoder outputs the parameters $\mu$ and $\sigma$.</li>
<li>Then sample latent code $z$ using $z = \mu +\sigma\cdot\epsilon$ where $\epsilon \sim N(0, I)$.</li>
<li>Then pass $z$ to the decoder.</li>
</ul>
<h3 id="training"><strong>Training</strong><a hidden class="anchor" aria-hidden="true" href="#training">#</a></h3>
<ol>
<li>Run input data $x$ through encoder to get distribution over $z$.</li>
<li>Use prior loss to enforce the encoder outputs to follow a unit Gaussian distribution (zero mean, unit variance).</li>
<li>Sample $z$ from encoder output $q_\phi(z\mid x)$ (Reparameterization trick).</li>
<li>Run $z$ through decoder to get predicted data mean (Reconstruction).</li>
<li>Use reconstruction loss to make predicted mean match $x$ under an L2 objective.</li>
</ol>
<h2 id="elaboration">Elaboration<a hidden class="anchor" aria-hidden="true" href="#elaboration">#</a></h2>
<h3 id="1-the-core-concept">1. The Core Concept<a hidden class="anchor" aria-hidden="true" href="#1-the-core-concept">#</a></h3>
<p>Unlike standard Autoencoders which map <code>Image</code> $\to$ <code>Code</code> $\to$ <code>Image</code>, a VAE maps <strong><code>Image</code> $\to$ <code>Distribution Parameters</code> $\to$ <code>Image</code></strong>.</p>
<p>The networks do not output probabilities directly; they output the <strong>parameters</strong> (Mean $\mu$ and Standard Deviation $\sigma$) of Gaussian distributions.</p>
<h3 id="2-the-encoder-inference-model">2. The Encoder (Inference Model)<a hidden class="anchor" aria-hidden="true" href="#2-the-encoder-inference-model">#</a></h3>
<p>The encoder compresses high-dimensional data into a low-dimensional latent space.</p>
<ul>
<li>
<p><strong>Input:</strong> Image of shape $3 \times H \times W$.</p>
</li>
<li>
<p><strong>Output:</strong> Two vectors, both of length <strong>$D$</strong> (the latent dimension, e.g., 128).</p>
<ol>
<li><strong>Mean Vector ($\mu_z$):</strong> The center of the latent distribution.</li>
<li><strong>Log-Variance Vector ($\log \sigma^2_z$):</strong> The spread of the distribution.</li>
</ol>
</li>
<li>
<p>Design Choice of using Diagonal Covariance:</p>
<p>We assume the dimensions of $z$ are statistically independent. Instead of predicting a full $D \times D$ covariance matrix (which would handle correlations between features), we only predict the diagonal.</p>
</li>
<li>
<p><strong>Intuition:</strong> This drastically reduces parameters from quadratic ($D^2$) to linear ($D$), making the model easier to train.</p>
</li>
</ul>
<h3 id="3-the-reparameterization-trick"><strong>3. The Reparameterization Trick</strong><a hidden class="anchor" aria-hidden="true" href="#3-the-reparameterization-trick">#</a></h3>
<p>The reparameterization trick is a tactical solution to a technical problem: <strong>How do we backpropagate through a random node?</strong></p>
<ul>
<li><strong>The Issue:</strong> Inside the network, we need to sample $z$ from the distribution $q_\phi(z|x)$ (typically a Gaussian with mean $\mu$ and variance $\sigma^2$. Standard random sampling breaks the chain of derivatives needed for backpropagation.</li>
<li><strong>The Trick:</strong> We move the randomness to an external variable $\epsilon$ that is independent of the model parameters.</li>
<li><strong>The Equation:</strong> Instead of sampling $z \sim N(\mu, \sigma^2)$ directly, we calculate:
$z = \mu + \sigma \odot \epsilon$ where $\epsilon \sim N(0, 1)$ (standard normal distribution).</li>
</ul>
<p>This allows gradients to flow through $\mu$ and $\sigma$ during training, making the VAE end-to-end differentiable.</p>
<h3 id="4-the-decoder-generative-model">4. The Decoder (Generative Model)<a hidden class="anchor" aria-hidden="true" href="#4-the-decoder-generative-model">#</a></h3>
<p>The decoder reconstructs the image from a sampled latent point.</p>
<ul>
<li>
<p><strong>Input:</strong> A vector $z$ of length <strong>$D$</strong> (sampled from the Encoder&rsquo;s distribution).</p>
</li>
<li>
<p><strong>Output:</strong> A tensor of shape <strong>$3 \times H \times W$</strong> (same as input image).</p>
</li>
<li>
<p>What this output represents:</p>
<p>Mathematically, this output is the Mean Vector ($\mu_x$) of the pixel probability distribution.</p>
</li>
<li>
<p>Design Choice (Fixed Variance):</p>
<p>We assume the pixel distribution is a Gaussian with a fixed standard deviation of 1 ($\sigma=1$) and spherical covariance (no correlations between pixels).</p>
</li>
</ul>
<h3 id="5-critical-intuition-why-design-it-this-way">5. Critical Intuition: Why Design it This Way?<a hidden class="anchor" aria-hidden="true" href="#5-critical-intuition-why-design-it-this-way">#</a></h3>
<p><strong>A. Why Diagonal/Fixed Covariance? (The &ldquo;Unmanageable Size&rdquo; Problem)</strong></p>
<ul>
<li>If the Decoder tried to learn the correlations between every pair of pixels (<strong>Full Covariance</strong>), it would need a matrix of size $(H \cdot W)^2$.</li>
<li>For a small image, this is millions of parameters; for large images, trillions.</li>
<li><strong>Solution:</strong> By assuming pixels are independent (Diagonal) or fixed (Spherical), we reduce complexity from <strong>Quadratic</strong> to <strong>Linear</strong>.</li>
</ul>
<p><strong>B. Why NOT Predict Separate Variances for Each Pixel? (The &ldquo;Cheating&rdquo; Problem)</strong></p>
<ul>
<li>
<p><strong>The Idea:</strong> Why not let the decoder output a specific variance $\sigma_i$ for every pixel $i$?</p>
</li>
<li>
<p>The Cheating Mechanism:</p>
<p>As derived in the Appendix, the Loss function includes two competing terms:</p>
$$
    \text{Loss} \approx \underbrace{\frac{(x - \mu)^2}{2\sigma^2}}_{\text{Reconstruction Error}} + \underbrace{\log(\sigma)}_{\text{Uncertainty Penalty}}
    $$<ul>
<li><strong>How it cheats:</strong> For difficult details (like edges), the reconstruction error $(x-\mu)^2$ is naturally high. To minimize the Loss, the model can simply predict a <strong>massive variance ($\sigma \to \infty$)</strong>.</li>
<li><strong>The Result:</strong> A huge $\sigma$ crushes the Reconstruction Error term to near zero. It is &ldquo;cheaper&rdquo; for the model to admit total uncertainty than to learn the hard feature.</li>
</ul>
</li>
<li>
<p><strong>The Fix:</strong> By forcing $\sigma=1$, the denominator is constant. The model <em>must</em> minimize $(x - \mu)^2$ to lower the loss.</p>
</li>
</ul>
<p><strong>C. Why is the &ldquo;Reconstructed Image&rdquo; just the Mean? (The MSE Connection)</strong></p>
<ul>
<li>
<p>When you maximize the likelihood of a Gaussian where $\sigma$ is fixed to 1, the math simplifies to:</p>
<p>$\text{Maximize } \log P(x|z) \iff \text{Minimize } (x - \mu)^2$</p>
</li>
<li>
<p>Therefore, the output $\mu$ is the value that minimizes the <strong>Mean Squared Error (L2 Loss)</strong>.</p>
</li>
</ul>
<h1 id="motivation-of-vae">Motivation of VAE<a hidden class="anchor" aria-hidden="true" href="#motivation-of-vae">#</a></h1>
<p>While standard auto-encoders are effective for learning feature representations (via reconstruction), they fail as generative models because their latent space ($Z$) has no enforced structure. Starting at <strong>[<a href="http://www.youtube.com/watch?v=zbHXQRUNlH0&amp;t=3078">51:18</a>].</strong></p>
<ul>
<li><strong>The Limitation of Autoencoders [<a href="http://www.youtube.com/watch?v=zbHXQRUNlH0&amp;t=3078">51:18</a>]:</strong> if you want to use a standard autoencoder to generate <em>new</em> data, you would need to throw away the encoder and sample a latent vector $Z$ to pass through the decoder. However, because the autoencoder places no constraints on the latent space, you have no idea what the distribution of valid $Z$ vectors looks like.</li>
<li><strong>&ldquo;Kicking the Can Down the Road&rdquo; [<a href="http://www.youtube.com/watch?v=zbHXQRUNlH0&amp;t=3117">51:57</a>]:</strong> just as we didn&rsquo;t know the distribution of the original data $X$, we now don&rsquo;t know the distribution of the latent vectors $Z$. Therefore, we are &ldquo;stuck&rdquo; because we cannot easily sample a valid code to generate a realistic image.</li>
<li><strong>The VAE Solution [<a href="http://www.youtube.com/watch?v=zbHXQRUNlH0&amp;t=3134">52:14</a>]:</strong> The motivation for the VAE is to &ldquo;force some structure on the $Z$&rsquo;s.&rdquo; By forcing the latent space to approximate a known distribution (typically a unit Gaussian), the VAE ensures that we <em>can</em> easily sample a random $Z$ from that known distribution and pass it through the decoder to generate valid new data.</li>
</ul>
<p>VAEs are essentially a &ldquo;probabilistic spin&rdquo; on traditional autoencoders designed specifically to enable this sampling capability <strong>[<a href="http://www.youtube.com/watch?v=zbHXQRUNlH0&amp;t=3188">53:08</a>]</strong>.</p>
<h1 id="formulation">Formulation<a hidden class="anchor" aria-hidden="true" href="#formulation">#</a></h1>
<h2 id="goal--maximize-marginal-likelihood">Goal — Maximize Marginal Likelihood<a hidden class="anchor" aria-hidden="true" href="#goal--maximize-marginal-likelihood">#</a></h2>
<p>We want to find parameters $\theta$ such that the data we observed ($x$) is highly probable under our model. We maximize the <strong>Marginal Likelihood</strong>, not the Conditional Likelihood.</p>
<ul>
<li>
<p>Equation: $\theta^* = \operatorname*{argmax}_\theta \sum_{i} \log p_\theta(x^{(i)})$</p>
</li>
<li>
<p>Why Marginal $p_\theta(x)$: The &ldquo;marginal&rdquo; integrates over <strong>all possible</strong> latent variables $z$.</p>
<ul>
<li><strong>$p(x\mid z)$ (Single Case):</strong> This asks, &ldquo;How likely is this image if the hidden concept is <strong>exactly</strong> $z$?&rdquo;</li>
<li><strong>$p(x)$ (Collectively all cases):</strong> This asks, &ldquo;How likely is this image considering <strong>every possible</strong> hidden concept that could produce it?&rdquo; i.e. $p(x) = \int p(x|z)p(z)dz$.</li>
</ul>
<table>
  <thead>
      <tr>
          <th><strong>Symbol</strong></th>
          <th><strong>Name</strong></th>
          <th><strong>Description</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>$p_\theta(x \mid z)$</strong></td>
          <td><strong>Likelihood</strong></td>
          <td>How likely is the image $x$, given the hidden traits $z$?</td>
      </tr>
      <tr>
          <td><strong>$p_\theta(z)$</strong></td>
          <td><strong>Prior</strong></td>
          <td>What is the distribution of hidden traits $z$ before seeing data?</td>
      </tr>
      <tr>
          <td><strong>$p_\theta(z \mid x)$</strong></td>
          <td><strong>Posterior</strong></td>
          <td>Given the image $x$, what are the likely traits $z$?</td>
      </tr>
      <tr>
          <td><strong>$p_\theta(x)$</strong></td>
          <td><strong>Marginal Likelihood</strong></td>
          <td>How probable is the data $x$ overall, summing over all possible $z$&rsquo;s?</td>
      </tr>
  </tbody>
</table>
</li>
<li>
<p>Why $p_\theta(x)$ is called marginal likelihood?</p>
<p>Historically, probability distributions were written in <strong>tables</strong>. For example, if you had a joint distribution over $x$ and $z$:</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>z=0</th>
          <th>z=1</th>
          <th><strong>Row total</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>x=0</td>
          <td>…</td>
          <td>…</td>
          <td><strong>p(x=0)</strong></td>
      </tr>
      <tr>
          <td>x=1</td>
          <td>…</td>
          <td>…</td>
          <td><strong>p(x=1)</strong></td>
      </tr>
      <tr>
          <td><strong>Column total</strong></td>
          <td><strong>p(z=0)</strong></td>
          <td><strong>p(z=1)</strong></td>
          <td>1</td>
      </tr>
  </tbody>
</table>
<p>To get the probability of $x$ alone, you would <strong>sum across the row</strong>, and the result would appear in the <strong>margin</strong> of the table. Hence: <strong>Summing out a variable = looking at the margin of the joint distribution.</strong> That operation was literally written in the <em>margins</em> of the table → “<strong>marginal</strong> probability.” The process of computing it → “<strong>marginalization</strong>.”</p>
</li>
</ul>
<h2 id="blocker--the-intractability">Blocker ← The Intractability<a hidden class="anchor" aria-hidden="true" href="#blocker--the-intractability">#</a></h2>
<p>To calculate the marginal likelihood, we face a mathematical dead end because we cannot compute the integral or the posterior.</p>
<ul>
<li>
<p>The Integral Problem: $p_\theta(x) = \int p_\theta(x|z)p(z) \, dz$</p>
<p>For complex data (like images), this integral is impossible to calculate (intractable) because it requires summing over an infinite number of possible $z$ configurations.</p>
</li>
<li>
<p>The Bayes&rsquo; Rule Problem:</p>
<p>We might try to find $p_\theta(x)$ via Bayes&rsquo; Rule:</p>
$$
    p_\theta(x) = \frac{p_\theta(x\mid z)p(z)}{p_\theta(z\mid x)}
    $$<p>However, we only have a decoder to compute $p_\theta(x\mid z)$. To compute the denominator $p_\theta(z|x)$ (the true posterior), we need to use the Bayes’ rule which requires knowing $p_\theta(x)$, $p_\theta(z\mid x) = \frac{p_\theta(x\mid z)p(z)}{p_\theta(x)}$, so it is intractable.</p>
</li>
</ul>
<h2 id="solution--variational-inference-replacingwith">Solution ← Variational Inference (Replacing $p_\theta$ with $q_\phi$)<a hidden class="anchor" aria-hidden="true" href="#solution--variational-inference-replacingwith">#</a></h2>
<p>Since the true posterior $p_\theta(z|x)$ is impossible to calculate, we <strong>approximate</strong> it with a tractable distribution $q_\phi(z|x)$ (the Encoder/Neural Network).</p>
<ul>
<li>The Approximation: $q_\phi(z|x) \approx p_\theta(z|x)$</li>
</ul>
<hr>
<h1 id="intuition">Intuition<a hidden class="anchor" aria-hidden="true" href="#intuition">#</a></h1>
<h2 id="the-reconstruction-loss-and-prior-loss-is-fighting-with-each-other"><strong>The reconstruction loss and prior loss is fighting with each other</strong><a hidden class="anchor" aria-hidden="true" href="#the-reconstruction-loss-and-prior-loss-is-fighting-with-each-other">#</a></h2>
<p><img alt="image.png" loading="lazy" src="/posts/vae-variational-auto-encoder/image.png"></p>
<p>This &ldquo;tug-of-war&rdquo; is one of the most fascinating parts of a VAE because you can actually <em>see</em> the result of these two forces fighting when you plot the latent space.</p>
<p>Here is the visual evidence that the KL divergence doesn&rsquo;t just &ldquo;force&rdquo; everything to zero, but rather organizes it.</p>
<h3 id="1-the-healthy-vae-balance">1. The &ldquo;Healthy&rdquo; VAE Balance<a hidden class="anchor" aria-hidden="true" href="#1-the-healthy-vae-balance">#</a></h3>
<p>When the Reconstruction Loss (keep data distinct) and KL Loss (keep data Gaussian) are balanced correctly, the latent space looks like this:</p>
<p><strong>What to notice:</strong></p>
<ul>
<li><strong>Global Centering:</strong> Notice that the <em>entire cloud</em> of points is centered around $(0,0)$ and roughly spans between $-3$ and $3$ (typical for a unit variance). This is the KL loss doing its job.</li>
<li><strong>Local Distinctness:</strong> Crucially, individual digits are <strong>not</strong> all collapsed to $(0,0)$. The &ldquo;7&quot;s might be clustered at $(-1, 2)$ and the &ldquo;0&quot;s at $(1, -1)$. The encoder has learned to shift the mean $\mu$ <em>away</em> from 0 just enough to distinguish the digits, but keeps them packed tight enough to satisfy the Gaussian prior.</li>
<li><strong>Smoothness:</strong> The clusters touch each other. This means if you sample a point halfway between a &ldquo;1&rdquo; and a &ldquo;7&rdquo;, you get a digit that looks like a plausible mix of both.</li>
</ul>
<hr>
<h3 id="2-what-happenswithoutthe-kl-force-standard-autoencoder">2. What happens <em>without</em> the KL &ldquo;Force&rdquo; (Standard Autoencoder)<a hidden class="anchor" aria-hidden="true" href="#2-what-happenswithoutthe-kl-force-standard-autoencoder">#</a></h3>
<p>If you remove the KL term (essentially predicting mean/variance but with no penalty for where they are), the Reconstruction Loss takes over completely.</p>
<p><strong>What to notice:</strong></p>
<ul>
<li><strong>Gaps and Explosions:</strong> The clusters fly apart. The model might put &ldquo;0&quot;s at $(100, 100)$ and &ldquo;1&quot;s at $(-50, -50)$ just to be absolutely sure it doesn&rsquo;t confuse them.</li>
<li><strong>No Structure:</strong> There is no center. The spread is arbitrary.</li>
<li><strong>Dead Zones:</strong> There are massive empty gaps between clusters. If you try to generate an image from those gaps, you get static/noise because the decoder has never seen data there.</li>
</ul>
<h3 id="3-what-happens-if-the-kl-force-wins-posterior-collapse">3. What happens if the KL Force Wins (Posterior Collapse)<a hidden class="anchor" aria-hidden="true" href="#3-what-happens-if-the-kl-force-wins-posterior-collapse">#</a></h3>
<p>This is the scenario you feared—where the model is &ldquo;forced&rdquo; to 0 and 1.</p>
<ul>
<li><strong>The Visual:</strong> Imagine the first plot, but all the colored clusters are stacked directly on top of each other at $(0,0)$.</li>
<li><strong>The Result:</strong> The encoder output is always $\mu=0, \sigma=1$ effectively ignoring the input image. The decoder receives pure noise every time and produces a single, blurry &ldquo;average&rdquo; image (like a gray ghost) for every single input.</li>
</ul>
<h3 id="4-summary">4. Summary<a hidden class="anchor" aria-hidden="true" href="#4-summary">#</a></h3>
<p>The KL divergence acts like a <strong>spring</strong> attached to the origin $(0,0)$.</p>
<ul>
<li><strong>Reconstruction</strong> tries to pull the data points apart so they don&rsquo;t overlap.</li>
<li><strong>KL Divergence</strong> (the spring) pulls them back toward the center.</li>
<li>The final state is a tense equilibrium: data points distinct enough to be recognized, but bunched tight enough to form a smooth, continuous space.</li>
</ul>
<h2 id="role-of--in-latent-code">Role of $z_i$ in latent code $Z$<a hidden class="anchor" aria-hidden="true" href="#role-of--in-latent-code">#</a></h2>
<p><img alt="Auto-Encoding Variational Bayes, ICLR 2014." loading="lazy" src="/posts/vae-variational-auto-encoder/image-1.png"></p>
<p>Auto-Encoding Variational Bayes, ICLR 2014 [4].</p>
<p>In a Variational Autoencoder (VAE), <strong>$Z$</strong> represents the latent code—a compressed, hidden representation of the input data. <strong>$z_i$</strong> refers to a specific individual dimension (or component) within this vector. The core idea shown here is <strong>disentanglement</strong>: the model attempts to map distinct, meaningful semantic features of the data to separate dimensions ($z_i$). For example, changing the value of one dimension ($z_1$) might smoothly transform the digit&rsquo;s identity (e.g., changing a 6 to a 0), while changing another dimension ($z_2$) might strictly alter the slant or thickness of the writing, without changing the digit itself.</p>
<p><img alt="Implementation of sampling $z_1$ and $z_2$ and set rest of $z_i$ to zeros." loading="lazy" src="/posts/vae-variational-auto-encoder/image-2.png"></p>
<p>Implementation of sampling $z_1$ and $z_2$ and set rest of $z_i$ to zeros.</p>
<h1 id="limitation-blurry-results-ie-whats-next">Limitation: Blurry results (i.e. What’s Next?)<a hidden class="anchor" aria-hidden="true" href="#limitation-blurry-results-ie-whats-next">#</a></h1>
<p>The nature of loss function (MSE/log-likelihood) tend to produce &ldquo;blurry&rdquo; results in VAE.</p>
<p>The &ldquo;blurry VAE&rdquo; phenomenon is a direct mathematical consequence of how we measure &ldquo;error&rdquo; using <strong>Mean Squared Error (MSE)</strong> or <strong>Log-Likelihood</strong>.</p>
<p>In short: <strong>MSE forces the model to be a conservative &ldquo;average,&rdquo; rather than a bold &ldquo;guesser.&rdquo;</strong></p>
<p>Here is the breakdown of why this happens.</p>
<h3 id="1-the-safety-of-the-average">1. The &ldquo;Safety&rdquo; of the Average<a hidden class="anchor" aria-hidden="true" href="#1-the-safety-of-the-average">#</a></h3>
<p>Imagine the model is trying to reconstruct a picture of a zebra, but the latent space is slightly uncertain about exactly where a specific stripe should be.</p>
<ul>
<li><strong>Possibility A:</strong> The stripe is at pixel 100.</li>
<li><strong>Possibility B:</strong> The stripe is at pixel 101.</li>
</ul>
<p>If the model guesses A (sharp stripe at 100), but the truth was B, the MSE penalty is massive because the pixel values are totally opposite (black vs. white).</p>
<p>If the model guesses B (sharp stripe at 101), but the truth was A, the penalty is essentially double (wrong on both pixels).</p>
<p><strong>The VAE&rsquo;s Solution:</strong> It predicts a <strong>gray smear</strong> across pixels 100 and 101.</p>
<ul>
<li><strong>Why?</strong> Gray is never &ldquo;perfectly right,&rdquo; but it is never &ldquo;catastrophically wrong.&rdquo; It minimizes the <em>squared</em> error across all plausible possibilities. The model learns to <strong>hedge its bets</strong> to lower the loss.</li>
</ul>
<h3 id="2-the-unimodal-assumption-mathematics">2. The Unimodal Assumption (Mathematics)<a hidden class="anchor" aria-hidden="true" href="#2-the-unimodal-assumption-mathematics">#</a></h3>
<p>This is the technical root of the problem.</p>
<ul>
<li><strong>MSE is equivalent to Maximum Likelihood under a Gaussian distribution.</strong></li>
<li>When you use MSE, you are implicitly telling the model: <em>&ldquo;Assume the pixel value comes from a single Bell curve (Unimodal).&rdquo;</em></li>
</ul>
<p>The Problem: Real data is Multimodal.</p>
<p>A pixel at the edge of an object could plausibly be Black (background) OR White (object). It is almost never Gray.</p>
<ul>
<li><strong>Multimodal Reality:</strong> Two peaks (one at 0, one at 255).</li>
<li><strong>Unimodal Constraint:</strong> The model must fit <em>one</em> Bell curve to explain both peaks.</li>
<li><strong>Result:</strong> The model centers the Bell curve right in the middle (Gray/Blur) to cover both options.</li>
</ul>
<h3 id="3-high-frequency-vs-low-frequency">3. High Frequency vs. Low Frequency<a hidden class="anchor" aria-hidden="true" href="#3-high-frequency-vs-low-frequency">#</a></h3>
<ul>
<li><strong>Low Frequency (Structure):</strong> Where is the head? Where is the background? VAEs are great at this because the &ldquo;average&rdquo; of a head is still roughly a head shape.</li>
<li><strong>High Frequency (Texture/Edges):</strong> Where is this specific hair strand? Where is the pore on the skin? The &ldquo;average&rdquo; of many possible hair strand positions is just a smooth blur. Since VAEs optimize for the average case, they effectively apply a <strong>low-pass filter</strong> to the image, smoothing out all the sharp &ldquo;high frequency&rdquo; noise that our eyes interpret as realistic detail.</li>
</ul>
<h3 id="comparison-why-gans-5-dont-blur">Comparison: Why GANs [5] don&rsquo;t blur<a hidden class="anchor" aria-hidden="true" href="#comparison-why-gans-5-dont-blur">#</a></h3>
<ul>
<li><strong>VAE (MSE):</strong> &ldquo;I must be close to the pixel values on average. I will be safe and blurry.&rdquo;</li>
<li><strong>GAN (Discriminator):</strong> &ldquo;I don&rsquo;t care if the stripe is at pixel 100 or 101, but if it&rsquo;s gray/blurry, the Discriminator will know it&rsquo;s fake. I must pick <strong>one</strong> sharp location, even if I guess wrong.&rdquo;</li>
</ul>
<table>
  <thead>
      <tr>
          <th><strong>Loss Function</strong></th>
          <th><strong>Strategy</strong></th>
          <th><strong>Result</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>MSE / L2</strong></td>
          <td>Minimize variance; fit the mean of the distribution.</td>
          <td><strong>Blurry.</strong> The mean of &ldquo;sharp left&rdquo; and &ldquo;sharp right&rdquo; is &ldquo;blurry middle.&rdquo;</td>
      </tr>
      <tr>
          <td><strong>L1 Loss</strong></td>
          <td>Minimize absolute error; fit the <em>median</em>.</td>
          <td><strong>Slightly sharper</strong>, but still blurry compared to GANs.</td>
      </tr>
      <tr>
          <td><strong>Adversarial</strong></td>
          <td>Fool a judge; match the <em>distribution</em>.</td>
          <td><strong>Sharp.</strong> Forces a decision (collapse to a mode) rather than an average.</td>
      </tr>
  </tbody>
</table>
<p><strong>The Perceptual Loss (VGG Loss)</strong> [6] or <strong>VQ-VAE</strong> [7] are the standard modern techniques specifically designed to fix this blurriness in VAEs without needing a full GAN setup.</p>
<h1 id="retrospection"><strong>Retrospection</strong><a hidden class="anchor" aria-hidden="true" href="#retrospection">#</a></h1>
<ul>
<li>
<p><strong>Question-1:</strong> In a VAE, does the KL term make each class (e.g., each digit 0–9) form its own Gaussian cluster in latent space near zero, such that the <em>mixture</em> of the 10 class-wise clusters matches a standard normal, and the class separability comes from different encoder-produced $\mu$ and $\sigma$?</p>
<p>Concretely, We sample $z$ from the encoder, $z \sim q_\phi(z|x)$, where $q_\phi(z|x)$ is a gaussian distribution, i.e. $z \sim N(\mu, \sigma^2)$. We use KL divergence loss to make $N(\mu, \sigma^2)$ look similar to a standard gaussian distribution. $z$ is a latent code (vector of length $D$). Say, we are reconstructing hand written digit of 10 classes (digit 0 ~ 9). The $z$ representing digit $0$ is a gaussian distribution, and the $z$ representing digit $1$ is another gaussian distribution. These cluster both locate near to zero mean. The totality of the 10 cluster conform to the standard gaussian distribution. The distinctiveness between each cluster comes from the difference in $\mu$ and $\sigma$ generated by the encoder.</p>
</li>
<li>
<p><strong>Answer: T</strong>he understanding is correct.</p>
<p><strong>In summary, the VAE is a Master of Compromise</strong></p>
<ul>
<li>The VAE does <strong>clustering</strong> (via the reconstruction term), where each class (digit) gets its own localized cluster defined by its mean $\mu$.</li>
<li>The VAE does <strong>regularization</strong> (via the KL term), ensuring that all these clusters stay packed together in a smooth, continuous space that collectively resembles the $N(\mu, \sigma^2)$ distribution.</li>
</ul>
<ol>
<li><strong>The Local View (Individual Input)</strong>
For any single input (e.g., a specific image of a digit &lsquo;2&rsquo;), the encoder outputs a specific $\mu$ and $\sigma$.
<ul>
<li>This defines a <strong>&ldquo;neighborhood&rdquo;</strong> in the latent space where that specific image lives.</li>
<li>By sampling $z$ from this neighborhood, the decoder learns that any point in this small area should look like <em>that specific</em> &lsquo;2&rsquo;.</li>
</ul>
</li>
<li><strong>The Class View (Group of Inputs belong to same class)</strong>
As you noted, all the &lsquo;2&rsquo;s will have their own $\mu$ and $\sigma$ values.
<ul>
<li>Because they all share similar visual features, the Reconstruction Loss naturally forces their $\mu$ values to be near each other.</li>
<li>This creates a <strong>cluster</strong> (a Gaussian Mixture component) for the digit &lsquo;2&rsquo;.</li>
</ul>
</li>
<li><strong>The Global View (The Prior)</strong>
The <strong>KL Divergence</strong> is the &ldquo;global supervisor.&rdquo; It doesn&rsquo;t care about the labels (0–9); it only sees the totality of all these neighborhoods.
<ul>
<li>It exerts a pull on <strong>every</strong> individual distribution to stay close to $0$ and have a spread near $1$.</li>
<li>The result is that the &ldquo;cloud of clusters&rdquo; (the totality) conforms to the standard Gaussian shape.</li>
</ul>
</li>
</ol>
<p><strong>A Helpful Mental Model: &ldquo;The Bubble Map&rdquo;</strong>
Imagine each input image is a <strong>bubble</strong>.</p>
<ul>
<li>The <strong>Reconstruction Loss</strong> wants the bubbles to be solid and distinct so the decoder can &ldquo;see&rdquo; the image clearly.</li>
<li>The <strong>KL Loss</strong> wants all the bubbles to move to the center of the map and be exactly the same size.</li>
<li><strong>The Training Process</strong> is the struggle to pack all these bubbles into a small, circular container (the Standard Normal Prior) without them overlapping so much that they lose their identity.</li>
</ul>
</li>
<li>
<p><strong>Question-2:</strong> What is the use of the randomness in VAE ?</p>
<p>Concretely, let’s say we are reconstructing hand written digit of 10 classes (digit 0 ~ 9).</p>
<p>Regarding the encoder, when given 10 input images of digit ‘1’, the output $\mu$ and $\sigma$ would be very close if not the same. Namely, the latent code $z$ of the image of digit ‘1’ belong to the same cluster. Am i right?</p>
<p>During inference, if we feed the same image of ‘1’ to the encoder, though the $\mu$ and $\sigma$  output by encoder is the same, due to the randomness of $\epsilon$, the output $z$ would be different. namely, the latent code for the same input image is not deterministic. Am i right? In contradiction, for auto-encoder, the latent code for the same input image is deterministic. What is the use of the randomness?</p>
</li>
<li>
<p><strong>Answer:</strong> Randomness is the heart of the VAE magic.</p>
<p><strong>1. Verification of Intuition</strong></p>
<p><strong>Point 1: Do 10 images of digit &lsquo;1&rsquo; cluster together?</strong></p>
<ul>
<li><strong>Yes, you are right.</strong></li>
<li>The encoder will map all distinct images of &lsquo;1&rsquo; to values of $\mu$ and $\sigma$ that are close to each other in the latent space.</li>
<li><em>Nuance:</em> They won&rsquo;t be <em>exactly</em> the same. One &lsquo;1&rsquo; might be slanted (mapping to slightly left in the cluster), and another might be bold (mapping to slightly right). The VAE captures these stylistic differences in the precise values of $\mu$.</li>
</ul>
<p><strong>Point 2: Is the latent code $z$ stochastic for the same image?</strong></p>
<ul>
<li><strong>Yes, you are right.</strong></li>
<li>If you feed the exact same image into a VAE multiple times (and you are using the sampling step), you will get a slightly different vector $z$ each time because of the random noise $\epsilon$.</li>
<li><em>Note:</em> In practical deployment (e.g., if using a VAE just to compress data), engineers often skip the sampling and just use $\mu$ to get a deterministic code. But strictly speaking, the mathematical definition of the VAE inference path involves this randomness.</li>
</ul>
<p><strong>2. The Core Question: What is the use of the randomness?</strong></p>
<p>You asked: <em>&ldquo;For auto-encoder, the latent code is deterministic. What is the use of the randomness?&rdquo;</em>
This is the most critical concept in VAEs. The randomness transforms the latent space from a <strong>discrete lookup table</strong> into a <strong>continuous landscape</strong>.
Here is the analogy: <strong>The Dot vs. The Bubble.</strong></p>
<p><strong>A. The Standard Autoencoder (The Dot)</strong></p>
<ul>
<li><strong>Mechanism:</strong> It maps an input image to a single, precise point (a dot) in space.</li>
<li><strong>The Problem:</strong> The decoder only learns to decode <em>that specific point</em>.</li>
<li><strong>The Consequence:</strong> If you sample a point just slightly next to that dot—in the &ldquo;empty space&rdquo; between a &lsquo;1&rsquo; and a &lsquo;2&rsquo;—the decoder has no idea what to do. It often produces garbage or static because it never learned to handle that specific coordinate during training. The latent space is full of &ldquo;holes.&rdquo;</li>
</ul>
<p><strong>B. The VAE (The Bubble)</strong></p>
<ul>
<li><strong>Mechanism:</strong> By predicting a mean $\mu$ and variance $\sigma$ and adding noise, the encoder maps the input image not to a dot, but to a <strong>cloud</strong> or <strong>bubble</strong> of probability.</li>
<li><strong>The Effect:</strong> During training, the decoder is forced to reconstruct the digit &lsquo;1&rsquo; not just from a single point $\mu$, but from <em>any</em> point sampled within that bubble $z$.</li>
<li><strong>The &ldquo;Use&rdquo; of Randomness:</strong>
<ol>
<li><strong>Forcing Continuity (Smoothness):</strong> Because the decoder must reconstruct a &lsquo;1&rsquo; from anywhere inside the bubble, it learns that points <em>near</em> each other should produce similar outputs. This eliminates the &ldquo;holes.&rdquo; If two bubbles (say, a &lsquo;1&rsquo; and a &lsquo;7&rsquo;) overlap slightly, the decoder learns to generate a hybrid digit in that overlapping region.</li>
<li><strong>Dense Packing:</strong> The KL divergence (the &ldquo;spring&rdquo; we discussed earlier) tries to pack these bubbles as close to the center as possible without crushing them. Because they are bubbles (taking up volume) and not dots (infinitely small), they fill up the latent space completely.</li>
</ol>
</li>
</ul>
<p>The randomness prevents the model from &ldquo;memorizing&rdquo; specific points. It forces the model to learn a <strong>region</strong> for each digit, ensuring that the latent space is smooth, continuous, and safe to sample from for generation.</p>
</li>
</ul>
<h1 id="appendix">Appendix<a hidden class="anchor" aria-hidden="true" href="#appendix">#</a></h1>
<h2 id="derivation-of-the-gaussian-loss">Derivation of the Gaussian Loss<a hidden class="anchor" aria-hidden="true" href="#derivation-of-the-gaussian-loss">#</a></h2>
<p>Here is the derivation of why the loss function looks the way it does, starting from the definition of the Gaussian distribution.</p>
<p><strong>1. The Gaussian Probability Density Function (PDF)</strong></p>
<p>For a single pixel value $x$, modeled by a Gaussian with mean $\mu$ and standard deviation $\sigma$:</p>
$$
P(x|\mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \cdot e^{-\frac{(x - \mu)^2}{2\sigma^2}}
$$<p><strong>2. Log-Likelihood</strong></p>
<p>To train models, we want to maximize the probability of the true data. We take the Logarithm to make the math easier (Log is monotonic, so maximizing Log($P$) is the same as maximizing $P$).</p>
$$
\log P(x) = \log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) + \log\left(e^{-\frac{(x - \mu)^2}{2\sigma^2}}\right)
$$<p>Using logarithm rules $\log(e^y) = y$ and $\log(1/a) = -\log(a)$):</p>
$$
\log P(x) = -\frac{1}{2}\log(2\pi\sigma^2) - \frac{(x - \mu)^2}{2\sigma^2}
$$<p><strong>3. Negative Log-Likelihood (The Loss)</strong></p>
<p>In Deep Learning, we minimize Loss, which is the <code>Negative Log-Likelihood</code>. We flip the signs:</p>
$$
\text{Loss} = \underbrace{\frac{1}{2}\log(2\pi)}_{\text{Constant}} + \underbrace{\log(\sigma)}_{\text{Variance Term}} + \underbrace{\frac{(x - \mu)^2}{2\sigma^2}}_{\text{Error Term}}
$$<p><strong>4. Simplified Loss for Analysis</strong></p>
<p>Ignoring the constant (since it doesn&rsquo;t change with weights), we get the equation used in the &ldquo;Cheating&rdquo; section:</p>
$$
\text{Loss} \propto \frac{(x - \mu)^2}{2\sigma^2} + \log(\sigma)
$$<h2 id="derivation-of-the-elbo-evidence-lower-bound">Derivation of the ELBO (Evidence Lower Bound)<a hidden class="anchor" aria-hidden="true" href="#derivation-of-the-elbo-evidence-lower-bound">#</a></h2>
<p>The lecture did a great job in explaining ELBO. [1:03:04 to 1:08:55]</p>
$$
\log p_\theta(x) = \log \frac{p_\theta(x|z) p(z)}{p_\theta(z|x)}
$$<p>Multiply top and bottom by $\textcolor{lightblue}{q_\phi(z|x)}$</p>
$$
\log p_\theta(x) = \log \frac{p_\theta(x|z)p(z)}{p_\theta(z|x)} = \log \frac{p_\theta(x|z)p(z)\textcolor{lightblue}{q_\phi(z|x)}}{p_\theta(z|x)\textcolor{lightblue}{q_\phi(z|x)}}
$$<p>Logarithms + rearranging:</p>
$$
\begin{align*} \log p_\theta(x) &= \log \frac{p_\theta(x \mid z)p(z)}{p_\theta(z \mid x)} = \log \frac{\textcolor{cyan}{p_\theta(x \mid z)}\textcolor{green}{p(z)}\textcolor{red}{q_\phi(z \mid x)}}{\textcolor{yellow}{p_\theta(z \mid x)}\textcolor{lightblue}{q_\phi(z \mid x)}} \\[12pt] &= \log \textcolor{cyan}{p_\theta(x \mid z)} - \log \frac{\textcolor{lightblue}{q_\phi(z \mid x)}}{\textcolor{green}{p(z)}} + \log \frac{\textcolor{red}{q_\phi(z \mid x)}}{\textcolor{yellow}{p_\theta(z \mid x)}} \end{align*}
$$<p>[1:04:01 - 1:04:58] We can wrap in an expectation since it doesn’t depend on $z$: $\log p_\theta(x) = E_{z \sim q_\phi(z \mid x)}\left[\log p_\theta(x)\right]$. <strong>The expectation (average) of a constant value is just the constant itself.</strong></p>
<ul>
<li>In the notation $E_{z \sim q_\phi(z \mid x)}\left[\log p_\theta(x)\right]$:
<ul>
<li>$z$ is indeed sampled from the distribution $q_\phi$.</li>
<li>It is conditioned on the input $x$ (&ldquo;given x&rdquo;).</li>
<li>$q_\phi(z \mid x)$ is the <strong>Encoder.</strong></li>
<li><strong>In plain English:</strong> &ldquo;We are going to calculate the average value of [whatever term follows] by trying out many different latent codes ($z$) that our Encoder thinks are likely for this specific image ($x$).&rdquo;</li>
</ul>
</li>
<li>The term they are looking at is <strong>logP(x)</strong>. This represents the probability of the image (data) occurring.</li>
<li>The variable <strong>$z$</strong> represents the latent code (hidden features).</li>
<li><strong>Crucially:</strong> $\log P(x)$ depends <em>only</em> on the data $x$. It does <strong>not</strong> depend on $z$.</li>
<li>Therefore, relative to $z$, the term $\log P(x)$ is a <strong>constant</strong>.</li>
</ul>
$$
\log p_\theta(x) = E_z[\log p_\theta(x \mid z)] - E_z \left[ \log \frac{q_\phi(z \mid x)}{p(z)} \right] + E_z \left[ \log \frac{q_\phi(z \mid x)}{p_\theta(z \mid x)} \right]
$$<p>The 2nd and 3rd term are KL divergence which measures dissimilarity between two probability distributions.</p>
$$
\begin{align*}
\log p_{\theta}(x) &= \log \frac{p_{\theta}(x \mid z)p(z)}{p_{\theta}(z \mid x)} = \log \frac{p_{\theta}(x \mid z)p(z)q_{\phi}(z \mid x)}{p_{\theta}(z \mid x)q_{\phi}(z \mid x)} \\
&= E_{z}\left[\log p_{\theta}(x \mid z)\right] - E_{z}\left[\log \frac{q_{\phi}(z \mid x)}{p(z)}\right] + E_{z}\left[\log \frac{q_{\phi}(z \mid x)}{p_{\theta}(z \mid x)}\right] \\
&= E_{z \sim q_{\phi}(z \mid x)}\left[\log p_{\theta}(x \mid z)\right] - D_{KL}\left(q_{\phi}(z \mid x), p(z)\right) + D_{KL}\left(q_{\phi}(z \mid x), p_{\theta}(z \mid x)\right)
\end{align*}
$$<p>$E_{z \sim q_{\phi}(z \mid x)}\left[\log p_{\theta}(x \mid z)\right]$: The data reconstruction term. x→encoder→decoder should reconstruct $x$. Can compute in closed form for Gaussians.</p>
<p>$D_{KL}\left(q_{\phi}(z \mid x), p(z)\right)$: The prior term. We are forcing the Encoder to organize its output (i.e. the hidden codes $z$) so that, overall, they form a nice, neat Standard Gaussian distribution. Can compute in closed form for Gaussians.</p>
<ul>
<li><strong>The &ldquo;Encoder Output&rdquo;:</strong> This is $q_\phi(z \mid x)$. When you feed an image $x$ into the encoder, it doesn&rsquo;t just give you a single point; it gives you a <strong>probability distribution</strong> (specifically, it predicts a mean $\mu$ and a variance $\sigma^2$ for that image).</li>
<li><strong>The &ldquo;Prior&rdquo;:</strong> This is $p(z)$. We assume this is a <strong>Standard Unit Gaussian</strong> (a bell curve centered at 0 with a width of 1).</li>
<li><strong>The &ldquo;Match&rdquo;:</strong> The goal of training is to minimize the difference (KL Divergence) between the two.</li>
</ul>
<p>$D_{KL}\left(q_{\phi}(z \mid x), p_{\theta}(z \mid x)\right)$: Posterior Approximation. Encoder output $q_\phi(z\mid x)$ should match $p_\theta(z\mid x)$. We cannot compute this for Gaussians.</p>
<ul>
<li>KL is ≥ 0, so we can drop it to get lower bound on likelihood.</li>
</ul>
<p>This is out VAE training objective. Jointly train encoder $q$ and decoder $p$ to maximize the variational lower bound on the data likelihood. Also called Evidence Lower Bound (ELBo)</p>
$$
\log p_{\theta}(x) \geq E_{z \sim q_{\phi}(z|x)} \left[ \log p_{\theta}(x|z) \right] - D_{KL} \left( q_{\phi}(z|x), p(z) \right)
$$<p><img alt="image.png" loading="lazy" src="/posts/vae-variational-auto-encoder/image-3.png"></p>
<hr>
<h1 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h1>
<p>Please cite this work as:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-latex" data-lang="latex"><span style="display:flex;"><span>Bai, Yechao. &#34;VAE Revisited 2026: The Foundation of Generative AI&#34;. Yechao&#39;s Log (Jan 2026). https://bai-yunhan.github.io/posts/vae-variational-auto-encoder
</span></span></code></pre></div><p>Or use the BibTex citation:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-latex" data-lang="latex"><span style="display:flex;"><span>@article{Bai2026VAE,
</span></span><span style="display:flex;"><span>  title = {VAE Revisited 2026: The Foundation of Generative AI},
</span></span><span style="display:flex;"><span>  author = {Bai, Yechao},
</span></span><span style="display:flex;"><span>  journal = {bai-yunhan.github.io},
</span></span><span style="display:flex;"><span>  year = {2026},
</span></span><span style="display:flex;"><span>  month = {Jan},
</span></span><span style="display:flex;"><span>  url = &#34;https://bai-yunhan.github.io/posts/vae-variational-auto-encoder/&#34;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<ol>
<li>
<p><strong>High-Resolution Image Synthesis with Latent Diffusion Models</strong> (Stable Diffusion)<br>
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.<br>
<a href="https://arxiv.org/abs/2112.10752">arXiv:2112.10752</a></p>
</li>
<li>
<p><strong>Stanford CS231N: Deep Learning for Computer Vision</strong> (Lecture 13, Spring 2025)<br>
<a href="https://www.youtube.com/watch?v=zbHXQRUNlH0">YouTube Link</a></p>
</li>
<li>
<p><strong>Deep Residual Learning for Image Recognition</strong> (ResNet)<br>
He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.<br>
<a href="https://arxiv.org/abs/1512.03385">arXiv:1512.03385</a></p>
</li>
<li>
<p><strong>Auto-Encoding Variational Bayes</strong> (Original VAE Paper)<br>
Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes. <em>International Conference on Learning Representations (ICLR)</em>.<br>
<a href="https://arxiv.org/abs/1312.6114">arXiv:1312.6114</a></p>
</li>
<li>
<p><strong>Generative Adversarial Nets</strong> (GAN)<br>
Goodfellow, I., et al. (2014). <em>Advances in Neural Information Processing Systems (NeurIPS)</em>.<br>
<a href="https://arxiv.org/abs/1406.2661">arXiv:1406.2661</a></p>
</li>
<li>
<p><strong>Perceptual Losses for Real-Time Style Transfer and Super-Resolution</strong><br>
Johnson, J., Alahi, A., &amp; Fei-Fei, L. (2016). <em>European Conference on Computer Vision (ECCV)</em>.<br>
<a href="https://arxiv.org/abs/1603.08155">arXiv:1603.08155</a></p>
</li>
<li>
<p><strong>Neural Discrete Representation Learning</strong> (VQ-VAE)<br>
van den Oord, A., Vinyals, O., &amp; Kavukcuoglu, K. (2017). <em>Advances in Neural Information Processing Systems (NeurIPS)</em>.<br>
<a href="https://arxiv.org/abs/1711.00937">arXiv:1711.00937</a></p>
</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://bai-yunhan.github.io/tags/generative/">Generative</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://bai-yunhan.github.io/">Yechao&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
