<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>VAE (Variational Auto Encoder) | Yechao&#39;s Blog</title>
<meta name="keywords" content="generative">
<meta name="description" content="Source: Stanford CS231N Deep Learning for Computer Vision | Spring 2025 | Lecture 13: Generative Models 1 (link)

Implementation
Pseudo Code
Encoder

The input image dimension is [$B$, $C$, $H$, $W$] in this case is [1, 28, 28], where 28x28 is the image size of MNIST data.
The original classification head (fc) is replaced to produce the mean (mu) and log-variance (logvar) required for the VAE&rsquo;s reparameterization trick.
The flow of encoder:Image â†’ Modified ResNet â†’ Project â†’ Chunk â†’ mu, logvar

$[1, 28, 28]$ $\xrightarrow{\displaystyle \textsf{ResNet}}$ [1, 1024] $\xrightarrow{\displaystyle \textsf{Project}}$ $[1, 256]$ $\xrightarrow{\displaystyle \textsf{Chunk}}$ $([1, 128], [1, 128])$



Decoder

The flow of decoder: Project -&gt; Reshape -&gt; Nx Conv&#43;Up
The input to the decoder is the latent code $z$. The latent dimension is [$B$, $D_{in}$] ($D_{in}$ default to 128)

First it passes through a MLP â†’ [$B$, $D_{out}$]
Reshape from [$B$, $D_{out}$] â†’ $[B, C_{init}, H_{init}, W_{init}]$, where $H_{init}=W_{init}$


Then it passes through N layers of Convolution &#43; Up-sampleing layer â†’ $[B, 1, H_{out}, W_{out}]$, where $H_{out}=W_{out}=28$

VAE

The VAE takes data from MNIST dataset then pass it through a ResNet encoder.
Then sample latent code $z$ using $z = \mu &#43;\sigma\cdot\epsilon$ where $\epsilon \sim N(0, I)$.
Then pass $z$ to the decoder.

Training

Run input data through encoder to get distribution over $z$.
Prior loss: Encoder output should be unit Gaussian (zero mean, unit variance).
Sample $z$ from encoder output $q_\phi(z\mid x)$ (Reparameterization trick).
Run $z$ through decoder to get predicted data mean.
Reconstruction loss: predicted mean should match $x$ in L2.

Elaboration
1. The Core Concept
Unlike standard Autoencoders which mapÂ ImageÂ $\to$Â CodeÂ $\to$Â Image, a VAE mapsÂ ImageÂ $\to$Â Distribution ParametersÂ $\to$Â Image.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/vae-variational-auto-encoder/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/vae-variational-auto-encoder/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"></script>

<script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "\\[", right: "\\]", display: true },
          { left: "\\(", right: "\\)", display: false },
          { left: "$", right: "$", display: false }
        ]
      });
    });
    </script>



</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Yechao&#39;s Blog (Alt + H)">Yechao&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      VAE (Variational Auto Encoder)
    </h1>
    <div class="post-meta"><span title='2026-01-14 00:00:00 +0000 UTC'>January 14, 2026</span>&nbsp;Â·&nbsp;<span>22 min</span>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#implementation" aria-label="Implementation">Implementation</a><ul>
                        
                <li>
                    <a href="#pseudo-code" aria-label="Pseudo Code">Pseudo Code</a><ul>
                        
                <li>
                    <a href="#encoder" aria-label="Encoder">Encoder</a></li>
                <li>
                    <a href="#decoder" aria-label="Decoder">Decoder</a></li>
                <li>
                    <a href="#vae" aria-label="VAE">VAE</a></li>
                <li>
                    <a href="#training" aria-label="Training">Training</a></li></ul>
                </li>
                <li>
                    <a href="#elaboration" aria-label="Elaboration">Elaboration</a><ul>
                        
                <li>
                    <a href="#1-the-core-concept" aria-label="1. The Core Concept">1. The Core Concept</a></li>
                <li>
                    <a href="#2-the-encoder-inference-model" aria-label="2. The Encoder (Inference Model)">2. The Encoder (Inference Model)</a></li>
                <li>
                    <a href="#3-the-reparameterization-trick" aria-label="3. The Reparameterization Trick">3. The Reparameterization Trick</a></li>
                <li>
                    <a href="#4-the-decoder-generative-model" aria-label="4. The Decoder (Generative Model)">4. The Decoder (Generative Model)</a></li>
                <li>
                    <a href="#5-critical-intuition-why-design-it-this-way" aria-label="5. Critical Intuition: Why Design it This Way?">5. Critical Intuition: Why Design it This Way?</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#motivation-of-vae" aria-label="Motivation of VAE">Motivation of VAE</a></li>
                <li>
                    <a href="#formulation" aria-label="Formulation">Formulation</a><ul>
                        
                <li>
                    <a href="#goal--maximize-marginal-likelihood" aria-label="Goal â€” Maximize Marginal Likelihood">Goal â€” Maximize Marginal Likelihood</a><ul>
                        
                <li>
                    <a href="#-summing-out-a-variable--looking-at-the-margin-of-the-joint-distribution" aria-label="ğŸ”¹ Summing out a variable = looking at the margin of the joint distribution.">ğŸ”¹ Summing out a variable = looking at the margin of the joint distribution.</a></li></ul>
                </li>
                <li>
                    <a href="#blocker--the-intractability" aria-label="Blocker â† The Intractability">Blocker â† The Intractability</a></li>
                <li>
                    <a href="#solution--variational-inference-replacingwith" aria-label="Solution â† Variational Inference (ReplacingÂ $p_\theta$Â withÂ $q_\phi$)">Solution â† Variational Inference (ReplacingÂ $p_\theta$Â withÂ $q_\phi$)</a></li></ul>
                </li>
                <li>
                    <a href="#intuition" aria-label="Intuition">Intuition</a><ul>
                        
                <li>
                    <a href="#the-reconstruction-loss-and-prior-loss-is-fighting-with-each-other" aria-label="The reconstruction loss and prior loss is fighting with each other">The reconstruction loss and prior loss is fighting with each other</a><ul>
                        
                <li>
                    <a href="#1-the-healthy-vae-balance" aria-label="1. The &ldquo;Healthy&rdquo; VAE Balance">1. The &ldquo;Healthy&rdquo; VAE Balance</a></li>
                <li>
                    <a href="#2-what-happenswithoutthe-kl-force-standard-autoencoder" aria-label="2. What happensÂ withoutÂ the KL &ldquo;Force&rdquo; (Standard Autoencoder)">2. What happensÂ withoutÂ the KL &ldquo;Force&rdquo; (Standard Autoencoder)</a></li>
                <li>
                    <a href="#3-what-happens-if-the-kl-force-wins-posterior-collapse" aria-label="3. What happens if the KL Force Wins (Posterior Collapse)">3. What happens if the KL Force Wins (Posterior Collapse)</a></li>
                <li>
                    <a href="#4-summary" aria-label="4. Summary">4. Summary</a></li></ul>
                </li>
                <li>
                    <a href="#role-of--in-latent-code" aria-label="Role of $z_i$ in latent code $Z$">Role of $z_i$ in latent code $Z$</a></li></ul>
                </li>
                <li>
                    <a href="#limitation-blurry-results-ie-whats-next" aria-label="Limitation: Blurry results (i.e. Whatâ€™s Next?)">Limitation: Blurry results (i.e. Whatâ€™s Next?)</a><ul>
                        <ul>
                        
                <li>
                    <a href="#1-the-safety-of-the-average" aria-label="1. The &ldquo;Safety&rdquo; of the Average">1. The &ldquo;Safety&rdquo; of the Average</a></li>
                <li>
                    <a href="#2-the-unimodal-assumption-mathematics" aria-label="2. The Unimodal Assumption (Mathematics)">2. The Unimodal Assumption (Mathematics)</a></li>
                <li>
                    <a href="#3-high-frequency-vs-low-frequency" aria-label="3. High Frequency vs. Low Frequency">3. High Frequency vs. Low Frequency</a></li>
                <li>
                    <a href="#comparison-why-gans-dont-blur" aria-label="Comparison: Why GANs don&rsquo;t blur">Comparison: Why GANs don&rsquo;t blur</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#q--a" aria-label="Q &amp; A:">Q &amp; A:</a><ul>
                        
                <li>
                    <a href="#question-1" aria-label="Question-1">Question-1</a></li>
                <li>
                    <a href="#question-2" aria-label="Question-2">Question-2</a><ul>
                        
                <li>
                    <a href="#1-verification-of-your-intuition" aria-label="1. Verification of Your Intuition">1. Verification of Your Intuition</a></li>
                <li>
                    <a href="#2-the-core-question-what-is-the-use-of-the-randomness" aria-label="2. The Core Question: What is the use of the randomness?">2. The Core Question: What is the use of the randomness?</a></li>
                <li>
                    <a href="#a-the-standard-autoencoder-the-dot" aria-label="A. The Standard Autoencoder (The Dot)">A. The Standard Autoencoder (The Dot)</a></li>
                <li>
                    <a href="#b-the-vae-the-bubble" aria-label="B. The VAE (The Bubble)">B. The VAE (The Bubble)</a></li>
                <li>
                    <a href="#short-answer" aria-label="Short Answer">Short Answer</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#appendix" aria-label="Appendix">Appendix</a><ul>
                        
                <li>
                    <a href="#derivation-of-the-gaussian-loss" aria-label="Derivation of the Gaussian Loss">Derivation of the Gaussian Loss</a></li>
                <li>
                    <a href="#derivation-of-the-elbo-evidence-lower-bound" aria-label="Derivation of the ELBO (Evidence Lower Bound)">Derivation of the ELBO (Evidence Lower Bound)</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Source: <strong>Stanford CS231N Deep Learning for Computer Vision | Spring 2025 | Lecture 13: Generative Models 1 (<a href="https://www.youtube.com/watch?v=zbHXQRUNlH0&amp;list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16&amp;index=13&amp;t=3005s">link</a>)</strong></p>
<hr>
<h1 id="implementation">Implementation<a hidden class="anchor" aria-hidden="true" href="#implementation">#</a></h1>
<h2 id="pseudo-code">Pseudo Code<a hidden class="anchor" aria-hidden="true" href="#pseudo-code">#</a></h2>
<h3 id="encoder"><strong>Encoder</strong><a hidden class="anchor" aria-hidden="true" href="#encoder">#</a></h3>
<ul>
<li>The input image dimension is [$B$, $C$, $H$, $W$] in this case is [1, 28, 28], where 28x28 is the image size of MNIST data.</li>
<li>The original classification head (<code>fc</code>) is replaced to produce the mean (mu) and log-variance (<code>logvar</code>) required for the VAE&rsquo;s reparameterization trick.</li>
<li>The flow of encoder:<code>Image â†’ Modified ResNet â†’ Project â†’ Chunk â†’</code> <code>mu</code>, <code>logvar</code>
<ul>
<li>$[1, 28, 28]$ $\xrightarrow{\displaystyle \textsf{ResNet}}$ [1, 1024] $\xrightarrow{\displaystyle \textsf{Project}}$ $[1, 256]$ $\xrightarrow{\displaystyle \textsf{Chunk}}$ $([1, 128], [1, 128])$</li>
</ul>
</li>
</ul>
<h3 id="decoder"><strong>Decoder</strong><a hidden class="anchor" aria-hidden="true" href="#decoder">#</a></h3>
<ul>
<li>The flow of decoder: <code>Project -&gt; Reshape -&gt; Nx Conv+Up</code></li>
<li>The input to the decoder is the latent code $z$. The latent dimension is [$B$, $D_{in}$] ($D_{in}$ default to 128)
<ol>
<li>First it passes through a MLP â†’ [$B$, $D_{out}$]</li>
<li>Reshape from [$B$, $D_{out}$] â†’ $[B, C_{init}, H_{init}, W_{init}]$, where $H_{init}=W_{init}$</li>
</ol>
</li>
<li>Then it passes through N layers of Convolution + Up-sampleing layer â†’ $[B, 1, H_{out}, W_{out}]$, where $H_{out}=W_{out}=28$</li>
</ul>
<h3 id="vae"><strong>VAE</strong><a hidden class="anchor" aria-hidden="true" href="#vae">#</a></h3>
<ul>
<li>The VAE takes data from MNIST dataset then pass it through a ResNet encoder.</li>
<li>Then sample latent code $z$ using $z = \mu +\sigma\cdot\epsilon$ where $\epsilon \sim N(0, I)$.</li>
<li>Then pass $z$ to the decoder.</li>
</ul>
<h3 id="training"><strong>Training</strong><a hidden class="anchor" aria-hidden="true" href="#training">#</a></h3>
<ol>
<li>Run input data through encoder to get distribution over $z$.</li>
<li>Prior loss: Encoder output should be unit Gaussian (zero mean, unit variance).</li>
<li>Sample $z$ from encoder output $q_\phi(z\mid x)$ (Reparameterization trick).</li>
<li>Run $z$ through decoder to get predicted data mean.</li>
<li>Reconstruction loss: predicted mean should match $x$ in L2.</li>
</ol>
<h2 id="elaboration">Elaboration<a hidden class="anchor" aria-hidden="true" href="#elaboration">#</a></h2>
<h3 id="1-the-core-concept">1. The Core Concept<a hidden class="anchor" aria-hidden="true" href="#1-the-core-concept">#</a></h3>
<p>Unlike standard Autoencoders which mapÂ <code>Image</code>Â $\to$Â <code>Code</code>Â $\to$Â <code>Image</code>, a VAE mapsÂ <strong><code>Image</code>Â $\to$Â <code>Distribution Parameters</code>Â $\to$Â <code>Image</code></strong>.</p>
<p>The networks do not output probabilities directly; they output theÂ <strong>parameters</strong>Â (MeanÂ $\mu$Â and Standard DeviationÂ $\sigma$) of Gaussian distributions.</p>
<h3 id="2-the-encoder-inference-model">2. The Encoder (Inference Model)<a hidden class="anchor" aria-hidden="true" href="#2-the-encoder-inference-model">#</a></h3>
<p>The encoder compresses high-dimensional data into a low-dimensional latent space.</p>
<ul>
<li>
<p><strong>Input:</strong>Â Image of shapeÂ $3 \times H \times W$.</p>
</li>
<li>
<p><strong>Output:</strong>Â Two vectors, both of lengthÂ <strong>$D$</strong>Â (the latent dimension, e.g., 128).</p>
<ol>
<li><strong>Mean Vector ($\mu_z$):</strong>Â The center of the latent distribution.</li>
<li><strong>Log-Variance Vector ($\log \sigma^2_z$):</strong>Â The spread of the distribution.</li>
</ol>
</li>
<li>
<p>Design Choice (Diagonal Covariance):</p>
<p>We assume the dimensions of $z$ are statistically independent. Instead of predicting a full $D \times D$ covariance matrix (which would handle correlations between features), we only predict the diagonal.</p>
</li>
<li>
<p><strong>Intuition:</strong>Â This drastically reduces parameters from quadratic ($D^2$) to linear ($D$), making the model easier to train.</p>
</li>
</ul>
<h3 id="3-the-reparameterization-trick"><strong>3. The Reparameterization Trick</strong><a hidden class="anchor" aria-hidden="true" href="#3-the-reparameterization-trick">#</a></h3>
<p>The reparameterization trick is a tactical solution to a technical problem: <strong>How do we backpropagate through a random node?</strong></p>
<ul>
<li><strong>The Issue:</strong> Inside the network, we need to sample $z$ from the distribution $q_\phi(z|x)$ (typically a Gaussian with mean $\mu$ and variance $\sigma^2$. Standard random sampling breaks the chain of derivatives needed for backpropagation.</li>
<li><strong>The Trick:</strong> We move the randomness to an external variable $\epsilon$ that is independent of the model parameters.</li>
<li><strong>The Equation:</strong> Instead of sampling $z \sim N(\mu, \sigma^2)$ directly, we calculate:
$z = \mu + \sigma \odot \epsilon$ where $\epsilon \sim N(0, 1)$ (standard normal distribution).</li>
</ul>
<p>This allows gradients to flow through $\mu$ and $\sigma$ during training, making the VAE end-to-end differentiable.</p>
<h3 id="4-the-decoder-generative-model">4. The Decoder (Generative Model)<a hidden class="anchor" aria-hidden="true" href="#4-the-decoder-generative-model">#</a></h3>
<p>The decoder reconstructs the image from a sampled latent point.</p>
<ul>
<li>
<p><strong>Input:</strong>Â A vectorÂ $z$Â of lengthÂ <strong>$D$</strong>Â (sampled from the Encoder&rsquo;s distribution).</p>
</li>
<li>
<p><strong>Output:</strong>Â A tensor of shapeÂ <strong>$3 \times H \times W$</strong>Â (same as input image).</p>
</li>
<li>
<p>What this output represents:</p>
<p>Mathematically, this output is the Mean Vector ($\mu_x$) of the pixel probability distribution.</p>
</li>
<li>
<p>Design Choice (Fixed Variance):</p>
<p>We assume the pixel distribution is a Gaussian with a fixed standard deviation of 1 ($\sigma=1$) and spherical covariance (no correlations between pixels).</p>
</li>
</ul>
<h3 id="5-critical-intuition-why-design-it-this-way">5. Critical Intuition: Why Design it This Way?<a hidden class="anchor" aria-hidden="true" href="#5-critical-intuition-why-design-it-this-way">#</a></h3>
<p><strong>A. Why Diagonal/Fixed Covariance? (The &ldquo;Unmanageable Size&rdquo; Problem)</strong></p>
<ul>
<li>If the Decoder tried to learn the correlations between every pair of pixels (<strong>Full Covariance</strong>), it would need a matrix of sizeÂ $(H \cdot W)^2$.</li>
<li>For a small image, this is millions of parameters; for large images, trillions.</li>
<li><strong>Solution:</strong>Â By assuming pixels are independent (Diagonal) or fixed (Spherical), we reduce complexity fromÂ <strong>Quadratic</strong>Â toÂ <strong>Linear</strong>.</li>
</ul>
<p><strong>B. Why NOT Predict Separate Variances for Each Pixel? (The &ldquo;Cheating&rdquo; Problem)</strong></p>
<ul>
<li>
<p><strong>The Idea:</strong>Â Why not let the decoder output a specific varianceÂ $\sigma_i$Â for every pixelÂ $i$?</p>
</li>
<li>
<p>The Cheating Mechanism:</p>
<p>As derived in the Appendix, the Loss function includes two competing terms:</p>
$$
    \text{Loss} \approx \underbrace{\frac{(x - \mu)^2}{2\sigma^2}}_{\text{Reconstruction Error}} + \underbrace{\log(\sigma)}_{\text{Uncertainty Penalty}}
    $$<ul>
<li><strong>How it cheats:</strong>Â For difficult details (like edges), the reconstruction errorÂ $(x-\mu)^2$Â is naturally high. To minimize the Loss, the model can simply predict aÂ <strong>massive variance ($\sigma \to \infty$)</strong>.</li>
<li><strong>The Result:</strong>Â A hugeÂ $\sigma$Â crushes the Reconstruction Error term to near zero. It is &ldquo;cheaper&rdquo; for the model to admit total uncertainty than to learn the hard feature.</li>
</ul>
</li>
<li>
<p><strong>The Fix:</strong>Â By forcingÂ $\sigma=1$, the denominator is constant. The modelÂ <em>must</em>Â minimizeÂ $(x - \mu)^2$Â to lower the loss.</p>
</li>
</ul>
<p><strong>C. Why is the &ldquo;Reconstructed Image&rdquo; just the Mean? (The MSE Connection)</strong></p>
<ul>
<li>
<p>When you maximize the likelihood of a Gaussian where $\sigma$ is fixed to 1, the math simplifies to:</p>
<p>$\text{Maximize } \log P(x|z) \iff \text{Minimize } (x - \mu)^2$</p>
</li>
<li>
<p>Therefore, the outputÂ $\mu$Â is the value that minimizes theÂ <strong>Mean Squared Error (L2 Loss)</strong>.</p>
</li>
</ul>
<h1 id="motivation-of-vae">Motivation of VAE<a hidden class="anchor" aria-hidden="true" href="#motivation-of-vae">#</a></h1>
<p>While standard auto-encoders are effective for learning feature representations (via reconstruction), they fail as generative models because their latent space ($Z$) has no enforced structure. starting atÂ <strong>[<a href="http://www.youtube.com/watch?v=zbHXQRUNlH0&amp;t=3078">51:18</a>].</strong></p>
<ul>
<li><strong>The Limitation of Autoencoders [<a href="http://www.youtube.com/watch?v=zbHXQRUNlH0&amp;t=3078">51:18</a>]:</strong>Â if you want to use a standard autoencoder to generateÂ <em>new</em>Â data, you would need to throw away the encoder and sample a latent vectorÂ $Z$Â to pass through the decoder. However, because the autoencoder places no constraints on the latent space, you have no idea what the distribution of validÂ $Z$Â vectors looks like.</li>
<li><strong>&ldquo;Kicking the Can Down the Road&rdquo; [<a href="http://www.youtube.com/watch?v=zbHXQRUNlH0&amp;t=3117">51:57</a>]:</strong>Â He uses this phrase to describe the problem: just as we didn&rsquo;t know the distribution of the original dataÂ $X$, we now don&rsquo;t know the distribution of the latent vectorsÂ $Z$. Therefore, we are &ldquo;stuck&rdquo; because we cannot easily sample a valid code to generate a realistic image.</li>
<li><strong>The VAE Solution [<a href="http://www.youtube.com/watch?v=zbHXQRUNlH0&amp;t=3134">52:14</a>]:</strong>Â The motivation for the VAE is to &ldquo;force some structure on the $Z$&rsquo;s.&rdquo; By forcing the latent space to approximate a known distribution (typically a unit Gaussian), the VAE ensures that weÂ <em>can</em>Â easily sample a randomÂ $Z$Â from that known distribution and pass it through the decoder to generate valid new data.</li>
</ul>
<p>VAEs are essentially a &ldquo;probabilistic spin&rdquo; on traditional autoencoders designed specifically to enable this sampling capabilityÂ <strong>[<a href="http://www.youtube.com/watch?v=zbHXQRUNlH0&amp;t=3188">53:08</a>]</strong>.</p>
<h1 id="formulation">Formulation<a hidden class="anchor" aria-hidden="true" href="#formulation">#</a></h1>
<h2 id="goal--maximize-marginal-likelihood">Goal â€” Maximize Marginal Likelihood<a hidden class="anchor" aria-hidden="true" href="#goal--maximize-marginal-likelihood">#</a></h2>
<p>We want to find parametersÂ $\theta$Â such that the data we observed ($x$) is highly probable under our model. We maximize theÂ <strong>Marginal Likelihood</strong>, not the Conditional Likelihood.</p>
<ul>
<li>
<p>Equation: $\theta^* = \operatorname*{argmax}_\theta \sum_{i} \log p_\theta(x^{(i)})$</p>
</li>
<li>
<p>Why Marginal $p_\theta(x)$: The &ldquo;marginal&rdquo; integrates overÂ <strong>all possible</strong>Â latent variablesÂ $z$.</p>
<ul>
<li><strong>$p(x\mid z)$ (Single Case):</strong> This asks, &ldquo;How likely is this image if the hidden concept is <strong>exactly</strong> $z$?&rdquo;</li>
<li><strong>$p(x)$ (Collectively all cases):</strong> This asks, &ldquo;How likely is this image considering <strong>every possible</strong> hidden concept that could produce it?&rdquo; i.e. $p(x) = \int p(x|z)p(z)dz$.</li>
</ul>
<table>
  <thead>
      <tr>
          <th><strong>Symbol</strong></th>
          <th><strong>Name</strong></th>
          <th><strong>Description</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>$p_\theta(x \mid z)$</strong></td>
          <td><strong>Likelihood</strong></td>
          <td>How likely is the imageÂ $x$, given the hidden traitsÂ $z$?</td>
      </tr>
      <tr>
          <td><strong>$p_\theta(z)$</strong></td>
          <td><strong>Prior</strong></td>
          <td>What is the distribution of hidden traitsÂ $z$Â before seeing data?</td>
      </tr>
      <tr>
          <td><strong>$p_\theta(z \mid x)$</strong></td>
          <td><strong>Posterior</strong></td>
          <td>Given the imageÂ $x$, what are the likely traitsÂ $z$?</td>
      </tr>
      <tr>
          <td><strong>$p_\theta(x)$</strong></td>
          <td><strong>Marginal Likelihood</strong></td>
          <td>How probable is the dataÂ $x$Â overall, summing over all possibleÂ $z$&rsquo;s?</td>
      </tr>
  </tbody>
</table>
</li>
<li>
<p>Why $p_\theta(x)$ is called marginal likelihood?</p>
<p>Historically, probability distributions were written in <strong>tables</strong>. For example, if you had a joint distribution over $x$ and $z$:</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>z=0</th>
          <th>z=1</th>
          <th><strong>Row total</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>x=0</td>
          <td>â€¦</td>
          <td>â€¦</td>
          <td><strong>p(x=0)</strong></td>
      </tr>
      <tr>
          <td>x=1</td>
          <td>â€¦</td>
          <td>â€¦</td>
          <td><strong>p(x=1)</strong></td>
      </tr>
      <tr>
          <td><strong>Column total</strong></td>
          <td><strong>p(z=0)</strong></td>
          <td><strong>p(z=1)</strong></td>
          <td>1</td>
      </tr>
  </tbody>
</table>
<p>To get the probability of $x$ alone, you would <strong>sum across the row</strong>, and the result would appear in the <strong>margin</strong> of the table. Hence:</p>
<h3 id="-summing-out-a-variable--looking-at-the-margin-of-the-joint-distribution">ğŸ”¹ Summing out a variable = looking at the <strong>margin</strong> of the joint distribution.<a hidden class="anchor" aria-hidden="true" href="#-summing-out-a-variable--looking-at-the-margin-of-the-joint-distribution">#</a></h3>
<p>That operation was literally written in the <em>margins</em> of the table â†’ â€œ<strong>marginal</strong> probability.â€
The process of computing it â†’ â€œ<strong>marginalization</strong>.â€</p>
</li>
</ul>
<h2 id="blocker--the-intractability">Blocker â† The Intractability<a hidden class="anchor" aria-hidden="true" href="#blocker--the-intractability">#</a></h2>
<p>To calculate the marginal likelihood, we face a mathematical dead end because we cannot compute the integral or the posterior.</p>
<ul>
<li>
<p>The Integral Problem: $p_\theta(x) = \int p_\theta(x|z)p(z) \, dz$</p>
<p>For complex data (like images), this integral is impossible to calculate (intractable) because it requires summing over an infinite number of possible $z$ configurations.</p>
</li>
<li>
<p>The Bayes&rsquo; Rule Problem:</p>
<p>We might try to find $p_\theta(x)$ via Bayes&rsquo; Rule:</p>
$$
    p_\theta(x) = \frac{p_\theta(x\mid z)p(z)}{p_\theta(z\mid x)}
    $$<p>However, we only have a decoder to compute $p_\theta(x\mid z)$. To compute the denominator $p_\theta(z|x)$ (the true posterior), we need to use the Bayesâ€™ rule which requires knowing $p_\theta(x)$, $p_\theta(z\mid x) = \frac{p_\theta(x\mid z)p(z)}{p_\theta(x)}$, so it is intractable.</p>
</li>
</ul>
<h2 id="solution--variational-inference-replacingwith">Solution â† Variational Inference (ReplacingÂ $p_\theta$Â withÂ $q_\phi$)<a hidden class="anchor" aria-hidden="true" href="#solution--variational-inference-replacingwith">#</a></h2>
<p>Since the true posteriorÂ $p_\theta(z|x)$Â is impossible to calculate, weÂ <strong>approximate</strong>Â it with a tractable distributionÂ $q_\phi(z|x)$Â (the Encoder/Neural Network).</p>
<ul>
<li>The Approximation: $q_\phi(z|x) \approx p_\theta(z|x)$</li>
</ul>
<hr>
<h1 id="intuition">Intuition<a hidden class="anchor" aria-hidden="true" href="#intuition">#</a></h1>
<h2 id="the-reconstruction-loss-and-prior-loss-is-fighting-with-each-other"><strong>The reconstruction loss and prior loss is fighting with each other</strong><a hidden class="anchor" aria-hidden="true" href="#the-reconstruction-loss-and-prior-loss-is-fighting-with-each-other">#</a></h2>
<p><img alt="image.png" loading="lazy" src="/notion-assets/image.png"></p>
<p>This &ldquo;tug-of-war&rdquo; is one of the most fascinating parts of a VAE because you can actuallyÂ <em>see</em>Â the result of these two forces fighting when you plot the latent space.</p>
<p>Here is the visual evidence that the KL divergence doesn&rsquo;t just &ldquo;force&rdquo; everything to zero, but rather organizes it.</p>
<h3 id="1-the-healthy-vae-balance">1. The &ldquo;Healthy&rdquo; VAE Balance<a hidden class="anchor" aria-hidden="true" href="#1-the-healthy-vae-balance">#</a></h3>
<p>When the Reconstruction Loss (keep data distinct) and KL Loss (keep data Gaussian) are balanced correctly, the latent space looks like this:</p>
<p><strong>What to notice:</strong></p>
<ul>
<li><strong>Global Centering:</strong>Â Notice that theÂ <em>entire cloud</em>Â of points is centered aroundÂ $(0,0)$Â and roughly spans betweenÂ $-3$ andÂ $3$Â (typical for a unit variance). This is the KL loss doing its job.</li>
<li><strong>Local Distinctness:</strong>Â Crucially, individual digits areÂ <strong>not</strong>Â all collapsed toÂ $(0,0)$. The &ldquo;7&quot;s might be clustered atÂ $(-1, 2)$Â and the &ldquo;0&quot;s atÂ $(1, -1)$. The encoder has learned to shift the meanÂ $\mu$Â <em>away</em>Â from 0 just enough to distinguish the digits, but keeps them packed tight enough to satisfy the Gaussian prior.</li>
<li><strong>Smoothness:</strong>Â The clusters touch each other. This means if you sample a point halfway between a &ldquo;1&rdquo; and a &ldquo;7&rdquo;, you get a digit that looks like a plausible mix of both.</li>
</ul>
<hr>
<h3 id="2-what-happenswithoutthe-kl-force-standard-autoencoder">2. What happensÂ <em>without</em>Â the KL &ldquo;Force&rdquo; (Standard Autoencoder)<a hidden class="anchor" aria-hidden="true" href="#2-what-happenswithoutthe-kl-force-standard-autoencoder">#</a></h3>
<p>If you remove the KL term (essentially predicting mean/variance but with no penalty for where they are), the Reconstruction Loss takes over completely.</p>
<p><strong>What to notice:</strong></p>
<ul>
<li><strong>Gaps and Explosions:</strong>Â The clusters fly apart. The model might put &ldquo;0&quot;s atÂ $(100, 100)$Â and &ldquo;1&quot;s atÂ $(-50, -50)$Â just to be absolutely sure it doesn&rsquo;t confuse them.</li>
<li><strong>No Structure:</strong>Â There is no center. The spread is arbitrary.</li>
<li><strong>Dead Zones:</strong>Â There are massive empty gaps between clusters. If you try to generate an image from those gaps, you get static/noise because the decoder has never seen data there.</li>
</ul>
<h3 id="3-what-happens-if-the-kl-force-wins-posterior-collapse">3. What happens if the KL Force Wins (Posterior Collapse)<a hidden class="anchor" aria-hidden="true" href="#3-what-happens-if-the-kl-force-wins-posterior-collapse">#</a></h3>
<p>This is the scenario you fearedâ€”where the model is &ldquo;forced&rdquo; to 0 and 1.</p>
<ul>
<li><strong>The Visual:</strong>Â Imagine the first plot, but all the colored clusters are stacked directly on top of each other atÂ $(0,0)$.</li>
<li><strong>The Result:</strong>Â The encoder output is alwaysÂ $\mu=0, \sigma=1$Â effectively ignoring the input image. The decoder receives pure noise every time and produces a single, blurry &ldquo;average&rdquo; image (like a gray ghost) for every single input.</li>
</ul>
<h3 id="4-summary">4. Summary<a hidden class="anchor" aria-hidden="true" href="#4-summary">#</a></h3>
<p>The KL divergence acts like aÂ <strong>spring</strong>Â attached to the originÂ $(0,0)$.</p>
<ul>
<li><strong>Reconstruction</strong>Â tries to pull the data points apart so they don&rsquo;t overlap.</li>
<li><strong>KL Divergence</strong>Â (the spring) pulls them back toward the center.</li>
<li>The final state is a tense equilibrium: data points distinct enough to be recognized, but bunched tight enough to form a smooth, continuous space.</li>
</ul>
<h2 id="role-of--in-latent-code">Role of $z_i$ in latent code $Z$<a hidden class="anchor" aria-hidden="true" href="#role-of--in-latent-code">#</a></h2>
<p><img alt="Auto-Encoding Variational Bayes, ICLR 2014." loading="lazy" src="/notion-assets/image%201.png"></p>
<p>Auto-Encoding Variational Bayes, ICLR 2014.</p>
<p>In a Variational Autoencoder (VAE),Â <strong>$Z$</strong>Â represents the latent codeâ€”a compressed, hidden representation of the input data.Â <strong>$z_i$</strong>Â refers to a specific individual dimension (or component) within this vector. The core idea shown here isÂ <strong>disentanglement</strong>: the model attempts to map distinct, meaningful semantic features of the data to separate dimensions ($z_i$). For example, changing the value of one dimension ($z_1$) might smoothly transform the digit&rsquo;s identity (e.g., changing a 6 to a 0), while changing another dimension ($z_2$) might strictly alter the slant or thickness of the writing, without changing the digit itself.</p>
<p><img alt="Implementation of sampling $z_1$ and $z_2$ and set rest of $z_i$ to zeros." loading="lazy" src="/notion-assets/image%202.png"></p>
<p>Implementation of sampling $z_1$ and $z_2$ and set rest of $z_i$ to zeros.</p>
<h1 id="limitation-blurry-results-ie-whats-next">Limitation: Blurry results (i.e. Whatâ€™s Next?)<a hidden class="anchor" aria-hidden="true" href="#limitation-blurry-results-ie-whats-next">#</a></h1>
<p>The nature of loss function (MSE/log-likelihood) tend to produce &ldquo;blurry&rdquo; results in VAE.</p>
<p>The &ldquo;blurry VAE&rdquo; phenomenon is a direct mathematical consequence of how we measure &ldquo;error&rdquo; usingÂ <strong>Mean Squared Error (MSE)</strong>Â orÂ <strong>Log-Likelihood</strong>.</p>
<p>In short:Â <strong>MSE forces the model to be a conservative &ldquo;average,&rdquo; rather than a bold &ldquo;guesser.&rdquo;</strong></p>
<p>Here is the breakdown of why this happens.</p>
<h3 id="1-the-safety-of-the-average">1. The &ldquo;Safety&rdquo; of the Average<a hidden class="anchor" aria-hidden="true" href="#1-the-safety-of-the-average">#</a></h3>
<p>Imagine the model is trying to reconstruct a picture of a zebra, but the latent space is slightly uncertain about exactly where a specific stripe should be.</p>
<ul>
<li><strong>Possibility A:</strong>Â The stripe is at pixel 100.</li>
<li><strong>Possibility B:</strong>Â The stripe is at pixel 101.</li>
</ul>
<p>If the model guesses A (sharp stripe at 100), but the truth was B, the MSE penalty is massive because the pixel values are totally opposite (black vs. white).</p>
<p>If the model guesses B (sharp stripe at 101), but the truth was A, the penalty is essentially double (wrong on both pixels).</p>
<p><strong>The VAE&rsquo;s Solution:</strong>Â It predicts aÂ <strong>gray smear</strong>Â across pixels 100 and 101.</p>
<ul>
<li><strong>Why?</strong>Â Gray is never &ldquo;perfectly right,&rdquo; but it is never &ldquo;catastrophically wrong.&rdquo; It minimizes theÂ <em>squared</em>Â error across all plausible possibilities. The model learns toÂ <strong>hedge its bets</strong>Â to lower the loss.</li>
</ul>
<h3 id="2-the-unimodal-assumption-mathematics">2. The Unimodal Assumption (Mathematics)<a hidden class="anchor" aria-hidden="true" href="#2-the-unimodal-assumption-mathematics">#</a></h3>
<p>This is the technical root of the problem.</p>
<ul>
<li><strong>MSE is equivalent to Maximum Likelihood under a Gaussian distribution.</strong></li>
<li>When you use MSE, you are implicitly telling the model:Â <em>&ldquo;Assume the pixel value comes from a single Bell curve (Unimodal).&rdquo;</em></li>
</ul>
<p>The Problem: Real data is Multimodal.</p>
<p>A pixel at the edge of an object could plausibly be Black (background) OR White (object). It is almost never Gray.</p>
<ul>
<li><strong>Multimodal Reality:</strong>Â Two peaks (one at 0, one at 255).</li>
<li><strong>Unimodal Constraint:</strong>Â The model must fitÂ <em>one</em>Â Bell curve to explain both peaks.</li>
<li><strong>Result:</strong>Â The model centers the Bell curve right in the middle (Gray/Blur) to cover both options.</li>
</ul>
<h3 id="3-high-frequency-vs-low-frequency">3. High Frequency vs. Low Frequency<a hidden class="anchor" aria-hidden="true" href="#3-high-frequency-vs-low-frequency">#</a></h3>
<ul>
<li><strong>Low Frequency (Structure):</strong>Â Where is the head? Where is the background? VAEs are great at this because the &ldquo;average&rdquo; of a head is still roughly a head shape.</li>
<li><strong>High Frequency (Texture/Edges):</strong>Â Where is this specific hair strand? Where is the pore on the skin? The &ldquo;average&rdquo; of many possible hair strand positions is just a smooth blur. Since VAEs optimize for the average case, they effectively apply aÂ <strong>low-pass filter</strong>Â to the image, smoothing out all the sharp &ldquo;high frequency&rdquo; noise that our eyes interpret as realistic detail.</li>
</ul>
<h3 id="comparison-why-gans-dont-blur">Comparison: Why GANs don&rsquo;t blur<a hidden class="anchor" aria-hidden="true" href="#comparison-why-gans-dont-blur">#</a></h3>
<ul>
<li><strong>VAE (MSE):</strong>Â &ldquo;I must be close to the pixel values on average. I will be safe and blurry.&rdquo;</li>
<li><strong>GAN (Discriminator):</strong>Â &ldquo;I don&rsquo;t care if the stripe is at pixel 100 or 101, but if it&rsquo;s gray/blurry, the Discriminator will know it&rsquo;s fake. I must pickÂ <strong>one</strong>Â sharp location, even if I guess wrong.&rdquo;</li>
</ul>
<table>
  <thead>
      <tr>
          <th><strong>Loss Function</strong></th>
          <th><strong>Strategy</strong></th>
          <th><strong>Result</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>MSE / L2</strong></td>
          <td>Minimize variance; fit the mean of the distribution.</td>
          <td><strong>Blurry.</strong>Â The mean of &ldquo;sharp left&rdquo; and &ldquo;sharp right&rdquo; is &ldquo;blurry middle.&rdquo;</td>
      </tr>
      <tr>
          <td><strong>L1 Loss</strong></td>
          <td>Minimize absolute error; fit theÂ <em>median</em>.</td>
          <td><strong>Slightly sharper</strong>, but still blurry compared to GANs.</td>
      </tr>
      <tr>
          <td><strong>Adversarial</strong></td>
          <td>Fool a judge; match theÂ <em>distribution</em>.</td>
          <td><strong>Sharp.</strong>Â Forces a decision (collapse to a mode) rather than an average.</td>
      </tr>
  </tbody>
</table>
<p><strong>The Perceptual Loss (VGG Loss)</strong>Â orÂ <strong>VQ-VAE</strong> are the standard modern techniques specifically designed to fix this blurriness in VAEs without needing a full GAN setup.</p>
<h1 id="q--a">Q &amp; A:<a hidden class="anchor" aria-hidden="true" href="#q--a">#</a></h1>
<h2 id="question-1">Question-1<a hidden class="anchor" aria-hidden="true" href="#question-1">#</a></h2>
<p>Please validate my understanding of VAE below:
We sample $z$ from the encoder, $z \sim q_\phi(z|x)$. $q_\phi(z|x)$ is a gaussian distribution, i.e. $z \sim N(\mu, \sigma^2)$. We use KL divergence loss to make $N(\mu, \sigma^2)$ look similar to a standard gaussian distribution. $z$ is a latent code (vector of length $D$).
Say, we are reconstructing hand written digit of 10 classes (digit 0 ~ 9). The $z$ representing digit $0$ is a gaussian distribution, and the $z$ representing digit $1$ is another gaussian distribution. These cluster both locate near to zero mean. The totality of the 10 cluster conform to the standard gaussian distribution. The distinctiveness between each cluster comes from the difference in $\mu$ and $\sigma$ generated by the encoder. is my understanding correct?</p>
<p>Answer:</p>
<p>My understanding are confirmed to be correct.</p>
<p><strong>In summary, the VAE is a Master of Compromise</strong></p>
<ul>
<li>The VAE doesÂ <strong>clustering</strong>Â (via the reconstruction term), where each class (digit) gets its own localized cluster defined by its meanÂ $\mu$.</li>
<li>The VAE doesÂ <strong>regularization</strong>Â (via the KL term), ensuring that all these clusters stay packed together in a smooth, continuous space that collectively resembles theÂ $N(\mu, \sigma^2)$ distribution.</li>
</ul>
<ol>
<li><strong>The Local View (Individual Input)</strong>
For any single input (e.g.,Â a specific image of a digit &lsquo;2&rsquo;),Â the encoder outputs a specificÂ $\mu$Â andÂ $\sigma$.
<ul>
<li>This defines aÂ <strong>&ldquo;neighborhood&rdquo;</strong>Â in the latent space where that specific image lives.</li>
<li>By samplingÂ $z$Â from this neighborhood,Â the decoder learns that any point in this small area should look likeÂ <em>that specific</em>Â &lsquo;2&rsquo;.</li>
</ul>
</li>
<li><strong>The Class View (Group of Inputs belong to same class)</strong>
As you noted,Â all the &lsquo;2&rsquo;s will have their ownÂ $\mu$Â andÂ $\sigma$Â values.
<ul>
<li>Because they all share similar visual features,Â the Reconstruction Loss naturally forces theirÂ $\mu$Â values to be near each other.</li>
<li>This creates aÂ <strong>cluster</strong>Â (a Gaussian Mixture component) for the digit &lsquo;2&rsquo;.</li>
</ul>
</li>
<li><strong>The Global View (The Prior)</strong>
TheÂ <strong>KL Divergence</strong>Â is the &ldquo;global supervisor.&rdquo; It doesn&rsquo;t care about the labels (0â€“9); it only sees the totality of all these neighborhoods.
<ul>
<li>It exerts a pull onÂ <strong>every</strong>Â individual distribution to stay close toÂ $0$Â and have a spread nearÂ $1$.</li>
<li>The result is that the &ldquo;cloud of clusters&rdquo; (the totality) conforms to the standard Gaussian shape.</li>
</ul>
</li>
</ol>
<p><strong>A Helpful Mental Model: &ldquo;The Bubble Map&rdquo;</strong>
Imagine each input image is aÂ <strong>bubble</strong>.</p>
<ul>
<li>TheÂ <strong>Reconstruction Loss</strong>Â wants the bubbles to be solid and distinct so the decoder can &ldquo;see&rdquo; the image clearly.</li>
<li>TheÂ <strong>KL Loss</strong>Â wants all the bubbles to move to the center of the map and be exactly the same size.</li>
<li><strong>The Training Process</strong>Â is the struggle to pack all these bubbles into a small,Â circular container (the Standard Normal Prior) without them overlapping so much that they lose their identity.</li>
</ul>
<h2 id="question-2">Question-2<a hidden class="anchor" aria-hidden="true" href="#question-2">#</a></h2>
<p>Say, we are reconstructing hand written digit of 10 classes (digit 0 ~ 9).</p>
<p>Regarding the encoder, when given 10 input images of digit â€˜1â€™, the output $\mu$ and $\sigma$ would be very close if not the same. Namely, the latent code $z$ of the image of digit â€˜1â€™ belong to the same cluster. am i right?</p>
<p>During inference, if we feed the same image of â€˜1â€™ to the encoder, though the $\mu$ and $\sigma$  output by encoder is the same, due to the randomness of $\epsilon$, the output $z$ would be different. namely, the latent code for the same input image is not deterministic. Am i right? In contradiction, for auto-encoder, the latent code for the same input image is deterministic. What is the use of the randomness?</p>
<p>Answer:</p>
<p>Let&rsquo;s validate your points and then dive into the &ldquo;why&rdquo; of the randomness, which is the heart of the VAE magic.</p>
<h3 id="1-verification-of-your-intuition">1. Verification of Your Intuition<a hidden class="anchor" aria-hidden="true" href="#1-verification-of-your-intuition">#</a></h3>
<p><strong>Point 1: Do 10 images of digit &lsquo;1&rsquo; cluster together?</strong></p>
<ul>
<li><strong>Yes, you are right.</strong></li>
<li>The encoder will map all distinct images of &lsquo;1&rsquo; to values ofÂ $\mu$Â andÂ $\sigma$Â that are close to each other in the latent space.</li>
<li><em>Nuance:</em>Â They won&rsquo;t beÂ <em>exactly</em>Â the same. One &lsquo;1&rsquo; might be slanted (mapping to slightly left in the cluster), and another might be bold (mapping to slightly right). The VAE captures these stylistic differences in the precise values ofÂ $\mu$.</li>
</ul>
<p><strong>Point 2: Is the latent codeÂ $z$Â stochastic for the same image?</strong></p>
<ul>
<li><strong>Yes, you are right.</strong></li>
<li>If you feed the exact same image into a VAE multiple times (and you are using the sampling step), you will get a slightly different vectorÂ $z$Â each time because of the random noiseÂ $\epsilon$.</li>
<li><em>Note:</em>Â In practical deployment (e.g., if using a VAE just to compress data), engineers often skip the sampling and just useÂ $\mu$Â to get a deterministic code. But strictly speaking, the mathematical definition of the VAE inference path involves this randomness.</li>
</ul>
<h3 id="2-the-core-question-what-is-the-use-of-the-randomness">2. The Core Question: What is the use of the randomness?<a hidden class="anchor" aria-hidden="true" href="#2-the-core-question-what-is-the-use-of-the-randomness">#</a></h3>
<p>You asked:Â <em>&ldquo;For auto-encoder, the latent code is deterministic. What is the use of the randomness?&rdquo;</em></p>
<p>This is the most critical concept in VAEs. The randomness transforms the latent space from aÂ <strong>discrete lookup table</strong> into aÂ <strong>continuous landscape</strong>.</p>
<p>Here is the analogy:Â <strong>The Dot vs. The Bubble.</strong></p>
<h3 id="a-the-standard-autoencoder-the-dot">A. The Standard Autoencoder (The Dot)<a hidden class="anchor" aria-hidden="true" href="#a-the-standard-autoencoder-the-dot">#</a></h3>
<ul>
<li><strong>Mechanism:</strong>Â It maps an input image to a single, precise point (a dot) in space.</li>
<li><strong>The Problem:</strong>Â The decoder only learns to decodeÂ <em>that specific point</em>.</li>
<li><strong>The Consequence:</strong>Â If you sample a point just slightly next to that dotâ€”in the &ldquo;empty space&rdquo; between a &lsquo;1&rsquo; and a &lsquo;2&rsquo;â€”the decoder has no idea what to do. It often produces garbage or static because it never learned to handle that specific coordinate during training. The latent space is full of &ldquo;holes.&rdquo;</li>
</ul>
<h3 id="b-the-vae-the-bubble">B. The VAE (The Bubble)<a hidden class="anchor" aria-hidden="true" href="#b-the-vae-the-bubble">#</a></h3>
<ul>
<li><strong>Mechanism:</strong>Â By predicting a mean ($\mu$) and variance ($\sigma$) and adding noise, the encoder maps the input image not to a dot, but to aÂ <strong>cloud</strong>Â orÂ <strong>bubble</strong>Â of probability.</li>
<li><strong>The Effect:</strong>Â During training, the decoder is forced to reconstruct the digit &lsquo;1&rsquo; not just from a single pointÂ $\mu$, but fromÂ <em>any</em>Â point sampled within that bubble ($z$).</li>
<li><strong>The &ldquo;Use&rdquo; of Randomness:</strong>
<ol>
<li><strong>Forcing Continuity (Smoothness):</strong>Â Because the decoder must reconstruct a &lsquo;1&rsquo; from anywhere inside the bubble, it learns that pointsÂ <em>near</em>Â each other should produce similar outputs. This eliminates the &ldquo;holes.&rdquo; If two bubbles (say, a &lsquo;1&rsquo; and a &lsquo;7&rsquo;) overlap slightly, the decoder learns to generate a hybrid digit in that overlapping region.</li>
<li><strong>Dense Packing:</strong>Â The KL divergence (the &ldquo;spring&rdquo; we discussed earlier) tries to pack these bubbles as close to the center as possible without crushing them. Because they are bubbles (taking up volume) and not dots (infinitely small), they fill up the latent space completely.</li>
</ol>
</li>
</ul>
<h3 id="short-answer">Short Answer<a hidden class="anchor" aria-hidden="true" href="#short-answer">#</a></h3>
<p>The randomness prevents the model from &ldquo;memorizing&rdquo; specific points. It forces the model to learn aÂ <strong>region</strong>Â for each digit, ensuring that the latent space is smooth, continuous, and safe to sample from for generation.</p>
<h1 id="appendix">Appendix<a hidden class="anchor" aria-hidden="true" href="#appendix">#</a></h1>
<h2 id="derivation-of-the-gaussian-loss">Derivation of the Gaussian Loss<a hidden class="anchor" aria-hidden="true" href="#derivation-of-the-gaussian-loss">#</a></h2>
<p>Here is the derivation of why the loss function looks the way it does, starting from the definition of the Gaussian distribution.</p>
<p><strong>1. The Gaussian Probability Density Function (PDF)</strong></p>
<p>For a single pixel value $x$, modeled by a Gaussian with mean $\mu$ and standard deviation $\sigma$:</p>
$$
P(x|\mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \cdot e^{-\frac{(x - \mu)^2}{2\sigma^2}}
$$<p><strong>2. Log-Likelihood</strong></p>
<p>To train models, we want to maximize the probability of the true data. We take the Logarithm to make the math easier (Log is monotonic, so maximizing Log($P$) is the same as maximizing $P$).</p>
$$
\log P(x) = \log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) + \log\left(e^{-\frac{(x - \mu)^2}{2\sigma^2}}\right)
$$<p>Using logarithm rules $\log(e^y) = y$ and $\log(1/a) = -\log(a)$):</p>
$$
\log P(x) = -\frac{1}{2}\log(2\pi\sigma^2) - \frac{(x - \mu)^2}{2\sigma^2}
$$<p><strong>3. Negative Log-Likelihood (The Loss)</strong></p>
<p>In Deep Learning, we minimize Loss, which is the <code>Negative Log-Likelihood</code>. We flip the signs:</p>
$$
\text{Loss} = \underbrace{\frac{1}{2}\log(2\pi)}_{\text{Constant}} + \underbrace{\log(\sigma)}_{\text{Variance Term}} + \underbrace{\frac{(x - \mu)^2}{2\sigma^2}}_{\text{Error Term}}
$$<p><strong>4. Simplified Loss for Analysis</strong></p>
<p>Ignoring the constant (since it doesn&rsquo;t change with weights), we get the equation used in the &ldquo;Cheating&rdquo; section:</p>
$$
\text{Loss} \propto \frac{(x - \mu)^2}{2\sigma^2} + \log(\sigma)
$$<h2 id="derivation-of-the-elbo-evidence-lower-bound">Derivation of the ELBO (Evidence Lower Bound)<a hidden class="anchor" aria-hidden="true" href="#derivation-of-the-elbo-evidence-lower-bound">#</a></h2>
<p>The lecture did a great job in explaining ELBO. [1:03:04 to 1:08:55]</p>
$$
\log p_\theta(x) = \log \frac{p_\theta(x|z) p(z)}{p_\theta(z|x)}
$$<p>Multiply top and bottom by $\textcolor{lightblue}{q_\phi(z|x)}$</p>
$$
\log p_\theta(x) = \log \frac{p_\theta(x|z)p(z)}{p_\theta(z|x)} = \log \frac{p_\theta(x|z)p(z)\textcolor{lightblue}{q_\phi(z|x)}}{p_\theta(z|x)\textcolor{lightblue}{q_\phi(z|x)}}
$$<p>Logarithms + rearranging:</p>
$$
\begin{align*} \log p_\theta(x) &= \log \frac{p_\theta(x \mid z)p(z)}{p_\theta(z \mid x)} = \log \frac{\textcolor{cyan}{p_\theta(x \mid z)}\textcolor{green}{p(z)}\textcolor{red}{q_\phi(z \mid x)}}{\textcolor{yellow}{p_\theta(z \mid x)}\textcolor{lightblue}{q_\phi(z \mid x)}} \\[12pt] &= \log \textcolor{cyan}{p_\theta(x \mid z)} - \log \frac{\textcolor{lightblue}{q_\phi(z \mid x)}}{\textcolor{green}{p(z)}} + \log \frac{\textcolor{red}{q_\phi(z \mid x)}}{\textcolor{yellow}{p_\theta(z \mid x)}} \end{align*}
$$<p>[1:04:01 - 1:04:58] We can wrap in an expectation since it doesnâ€™t depend on $z$: $\log p_\theta(x) = E_{z \sim q_\phi(z \mid x)}\left[\log p_\theta(x)\right]$. <strong>The expectation (average) of a constant value is just the constant itself.</strong></p>
<ul>
<li>In the notationÂ $E_{z \sim q_\phi(z \mid x)}\left[\log p_\theta(x)\right]$:
<ul>
<li>$z$Â is indeed sampled from the distributionÂ $q_\phi$.</li>
<li>It is conditioned on the inputÂ $x$Â (&ldquo;givenÂ x&rdquo;).</li>
<li>$q_\phi(z \mid x)$Â is theÂ <strong>Encoder.</strong></li>
<li><strong>In plain English:</strong>Â &ldquo;We are going to calculate the average value of [whatever term follows] by trying out many different latent codes ($z$) that our Encoder thinks are likely for this specific image ($x$).&rdquo;</li>
</ul>
</li>
<li>The term they are looking at isÂ <strong>logP(x)</strong>. This represents the probability of the image (data) occurring.</li>
<li>The variableÂ <strong>$z$</strong>Â represents the latent code (hidden features).</li>
<li><strong>Crucially:</strong>Â $\log P(x)$Â dependsÂ <em>only</em>Â on the dataÂ $x$. It doesÂ <strong>not</strong>Â depend onÂ $z$.</li>
<li>Therefore, relative toÂ $z$, the termÂ $\log P(x)$Â is aÂ <strong>constant</strong>.</li>
</ul>
$$
\log p_\theta(x) = E_z[\log p_\theta(x \mid z)] - E_z \left[ \log \frac{q_\phi(z \mid x)}{p(z)} \right] + E_z \left[ \log \frac{q_\phi(z \mid x)}{p_\theta(z \mid x)} \right]
$$<p>The 2nd and 3rd term are KL divergence which measures dissimilarity between two probability distributions.</p>
$$
\begin{align*}
\log p_{\theta}(x) &= \log \frac{p_{\theta}(x \mid z)p(z)}{p_{\theta}(z \mid x)} = \log \frac{p_{\theta}(x \mid z)p(z)q_{\phi}(z \mid x)}{p_{\theta}(z \mid x)q_{\phi}(z \mid x)} \\
&= E_{z}\left[\log p_{\theta}(x \mid z)\right] - E_{z}\left[\log \frac{q_{\phi}(z \mid x)}{p(z)}\right] + E_{z}\left[\log \frac{q_{\phi}(z \mid x)}{p_{\theta}(z \mid x)}\right] \\
&= E_{z \sim q_{\phi}(z \mid x)}\left[\log p_{\theta}(x \mid z)\right] - D_{KL}\left(q_{\phi}(z \mid x), p(z)\right) + D_{KL}\left(q_{\phi}(z \mid x), p_{\theta}(z \mid x)\right)
\end{align*}
$$<p>$E_{z \sim q_{\phi}(z \mid x)}\left[\log p_{\theta}(x \mid z)\right]$: The data reconstruction term. xâ†’encoderâ†’decoder should reconstruct $x$. Can compute in closed form for Gaussians.</p>
<p>$D_{KL}\left(q_{\phi}(z \mid x), p(z)\right)$: The prior term. We are forcing the Encoder to organize its output (i.e. the hidden codes $z$) so that, overall, they form a nice, neat Standard Gaussian distribution. Can compute in closed form for Gaussians.</p>
<ul>
<li><strong>The &ldquo;Encoder Output&rdquo;:</strong> This isÂ $q_\phi(z \mid x)$. When you feed an imageÂ $x$Â into the encoder, it doesn&rsquo;t just give you a single point; it gives you a <strong>probability distribution</strong> (specifically, it predicts a meanÂ $\mu$Â and a varianceÂ $\sigma^2$Â for that image).</li>
<li><strong>The &ldquo;Prior&rdquo;:</strong>Â This isÂ $p(z)$. We assume this is aÂ <strong>Standard Unit Gaussian</strong>Â (a bell curve centered at 0 with a width of 1).</li>
<li><strong>The &ldquo;Match&rdquo;:</strong>Â The goal of training is to minimize the difference (KL Divergence) between the two.</li>
</ul>
<p>$D_{KL}\left(q_{\phi}(z \mid x), p_{\theta}(z \mid x)\right)$: Posterior Approximation. Encoder output $q_\phi(z\mid x)$ should match $p_\theta(z\mid x)$. We cannot compute this for Gaussians.</p>
<ul>
<li>KL is â‰¥ 0, so we can drop it to get lower bound on likelihood.</li>
</ul>
<p>This is out VAE training objective. Jointly train encoder $q$ and decoder $p$ to maximize the variational lower bound on the data likelihood. Also called Evidence Lower Bound (ELBo)</p>
$$
\log p_{\theta}(x) \geq E_{z \sim q_{\phi}(z|x)} \left[ \log p_{\theta}(x|z) \right] - D_{KL} \left( q_{\phi}(z|x), p(z) \right)
$$<p><img alt="image.png" loading="lazy" src="/notion-assets/image%203.png"></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/generative/">Generative</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">Yechao&#39;s Blog</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
